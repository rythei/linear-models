{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ec0ac2",
   "metadata": {},
   "source": [
    "## Computational Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9231d1",
   "metadata": {},
   "source": [
    "## Part 1: Cross Validation\n",
    "\n",
    "In this part, we will explore a technique called _cross-validation_ that can be used to estimate the regularization parameter $\\lambda$ in techniques like Ridge regression and LASSO. \n",
    "\n",
    "For this, we will need to download the file `EGFR_bioactivity.pkl` from [this link](https://drive.google.com/drive/folders/1OkXMcFo0urN0kSQYH4d75I4V3pnSpV6H?usp=sharing).\n",
    "\n",
    "Once downloaded (and saved in the same directory as this notebook), the file can be loaded using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28077331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 512), (2000,))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "with open('EGFR_bioactivity.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275bafa",
   "metadata": {},
   "source": [
    "The `EGFR_bioactivity` dataset contains bioactivity data for $n = 2000$ different drugs, each represented by a molecular \"fingerprints\", which here is a binary vector of length $p = 512$ indicating in each dimension whether the molecule contains a particular substructure or not. The response `y` is a measurement of how active each drug is against a particular protein (called EGFR) that it is targeting. The goal with this dataset is to build a model to predict this bioactivity from the features of the molecules.\n",
    "\n",
    "To do this, we will try fitting a Ridge regression model, as discussed in lab. As we've seen, fitting a Ridge regression model requires choosing a regularization parameter $\\lambda$, which we will do using a technique called cross-validation. The core idea with cross-validation is that we should pick the value of $\\lambda$ that achieves the smallest error rate on a held-out set of data not present in the data used to fit the model.\n",
    "\n",
    "Generically, ($k$-fold) cross-validation works as follows: \n",
    "\n",
    "- Fix an integer $0<k\\leq n$, and split the dataset into $k$ non-overlapping, equally sized groups $S_1,\\dots,S_k$.\n",
    "- Loop through each of the $k$ groups. At step $j$, fit the model on all of the data _except_ those datapoint in group $j$ to obtain parameters $\\hat{\\beta}_{(-j)}(\\lambda)$. Then, use this model to make predictions on the $j$th group of data: $\\hat{y}^{(-j)}_i = x_i\\cdot \\hat{\\beta}_{(-j)}(\\lambda)$ for $i \\in S_j$ (i.e. the points that weren't in the data used for fitting the model). Calculate the MSE of these predictions $\\text{MSE}_{j}(\\lambda) = \\sum_{i \\in S_j} (\\hat{y}_i^{(-j)} - y_i)^2$.\n",
    "- After fitting and evaluating models on all $k$ subsets, compute the mean error across each of the $k$ subsets: $\\text{MSE}(\\lambda) = \\frac{1}{k}\\sum_{j=1}^k \\text{MSE}_j(\\lambda)$. This is called the _cross-validation error_.\n",
    "\n",
    "To perform the splitting of the data, we can use the following function (most of the work here is being done by the function `np.array_split`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "238f4cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def k_fold_split(n=1000, k=5):\n",
    "    ix = range(n)\n",
    "    test_ix = np.array_split(ix, k)\n",
    "    train_ix = [[j for j in ix if j not in tix] for tix in test_ix]\n",
    "    return train_ix, test_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012b683",
   "metadata": {},
   "source": [
    "Below is an example of how we could use this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7ba65c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,) (400,)\n",
      "(1600,) (400,)\n",
      "(1600,) (400,)\n",
      "(1600,) (400,)\n",
      "(1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "train_ix, test_ix = k_fold_split(n = X.shape[0], k=k)\n",
    "\n",
    "for tr_ix, te_ix in zip(train_ix, test_ix):\n",
    "    X_train_curr, X_test_curr = X[tr_ix], X[te_ix]\n",
    "    y_train_curr, y_test_curr = y[tr_ix], y[te_ix]\n",
    "    print(y_train_curr.shape, y_test_curr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94447e3",
   "metadata": {},
   "source": [
    "Here, we set $k=5$, and looped through each of the folds, generating a training and testing set at each step, which is what we need to be able to do the $k$-fold cross validation strategy above.\n",
    "\n",
    "**Problem 1.** Use $5$-fold cross validation to estimate the cross validation error for a Ridge regression model fit with a fixed regularization parameter $\\lambda = 5$. (See the [online book](https://rythei.github.io/linear-models/content/generalizing_linear_regression/ridge_and_lasso.html) for example of how to fit such a model.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f272e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c6fb0b7",
   "metadata": {},
   "source": [
    "**Problem 2.** Now that we've developed a method to estimate the cross-validation error for a model with a fixed $\\lambda$, do the following: repeat the steps in problem 1 for a grid of $\\lambda$ values (for this problem a good range might be `np.exp(np.linspace(-3,7,10))`), and for each use 5-fold cross validation to estimate the cross-validation error. Plot these errors as a function of $\\lambda$, and report the value of $\\lambda$ that has the smallest error. (Note since we are using a log scale, the plot might look nicer if you use the option `plt.xscale(\"log\")`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ba1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6483172a",
   "metadata": {},
   "source": [
    "## Part 2: LASSO for variable selection\n",
    "\n",
    "In this problem, we will investigate using LASSO regression to perform variable selection, i.e. to automatically select a subset of the most important features. To fit LASSO models, you can use the `sklearn` package in python (see the [online book](https://rythei.github.io/linear-models/content/generalizing_linear_regression/ridge_and_lasso.html) for examples of how to do this).\n",
    "\n",
    "For this problem, we'll work with the California housing dataset, in which the goal is to predict the median house value in given region of California using various features. Once downloaded, the data can be loaded and processed using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "201ba30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"california_housing.csv\")\n",
    "y = dataset[\"MedHouseVal\"].to_numpy()\n",
    "feature_cols = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]\n",
    "X = dataset[feature_cols].to_numpy()\n",
    "ones = np.ones(shape=(X.shape[0], 1))\n",
    "feature_cols = [\"Intercept\"] + feature_cols\n",
    "X = np.hstack([ones, X])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f98c96",
   "metadata": {},
   "source": [
    "**Problem 3.** Using a range of $\\lambda$ values (say `np.exp(np.linspace(-4, 1, 5))`), compute the LASSO solution for this dataset, and for each value of $\\lambda$ report which coefficients are non-zero (and what features they are associated with). How many features are kept for the largest value of $\\lambda$? Which features are these? If you had to use this method, what would you say are the three most \"important\" features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b86c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afa5df8d",
   "metadata": {},
   "source": [
    "**Problem 4.** Similar to problem 2, use 5-fold cross-validation to select the best value of $\\lambda$ for the LASSO model, and plot the cross-validation errors as a function of $\\lambda$. What value of $\\lambda$ do you pick? Which features are selected (i.e. have non-zero coefficients) at this value of $\\lambda$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386a150b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
