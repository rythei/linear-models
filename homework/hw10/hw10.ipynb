{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4296c898",
   "metadata": {},
   "source": [
    "# Computational Homework 10: Fun with the eigenvalue decomposition\n",
    "\n",
    "## Part 1: The PageRank algorithm\n",
    "In this homework assignment, we investigate the original ranking algorithm developed by Larry Page and Sergey Brin, the founders of Google, to rank web pages based on importance.\n",
    "\n",
    "The basic setup of the algorithm is as follows: suppose a user searches for the term \"linear algebra\", and we find $N$ webpages containing a string match for this term. How do we determine which of these $N$ pages are most relevant? To solve this problem, the PageRank algorithm makes the following assumption: more important webpages are more likely to have other pages linking to it. \n",
    "\n",
    "We represent the stucture of interlinking webpages as a _graph_, i.e. an object consisting of $N$ vertices (webpages) and edges. There is an edge connecting vertex $i$ to vertex $j$ page $i$ links to page $j$. The structure of such a graph can be represented by an $N\\times N$ _adjacency matrix_ $\\boldsymbol{L}$ such that \n",
    "\n",
    "$$\n",
    "L_{ij} = \\begin{cases}1 & \\text{if $j$ links to $i$}\\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "(by default we always assume $L_{ii} = 0$). Consider the following simple example with four webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d93b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in /Users/rythei/opt/miniconda3/lib/python3.9/site-packages (2.8)\n"
     ]
    }
   ],
   "source": [
    "# you need networkx installed to visualize the graphs\n",
    "!pip install networkx\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462e8a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph with adjacency matrix\n",
      "\n",
      "[[0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 0 0]]\n",
      "\n",
      "can be visualized as\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = np.array([[0, 0, 1, 0], [1, 0, 0, 0], [1, 1, 0, 1], [0, 0, 0, 0]])\n",
    "print('The graph with adjacency matrix')\n",
    "print('')\n",
    "print(L)\n",
    "print('')\n",
    "print('can be visualized as')\n",
    "\n",
    "G = nx.from_numpy_matrix(L.T, create_using=nx.DiGraph)\n",
    "nx.draw(G, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a356b1e",
   "metadata": {},
   "source": [
    "Given a set of $N$ pages, the PageRank is a vector $\\boldsymbol{p}\\in \\mathbb{R}^N$, such that $\\boldsymbol{1}^\\top \\boldsymbol{p} = \\sum_{i=1}^N p_i = N$, where here $\\boldsymbol{1}$ is the all ones vector. The goal is to find $\\boldsymbol{p}$ such that the most important pages have the largest entry in the PageRank vector. We can now describe the actual PageRank algorithm for finding $\\boldsymbol{p}$.\n",
    "\n",
    "The total number of output links from page $j$ is given by $d_j = \\sum_{i=1}^N L_{ij}$. The PageRank algorithm defines the PageRanks as satisfying the following relation:\n",
    "\n",
    "$$\n",
    "p_i = (1-\\alpha) + \\alpha \\sum_{j=1}^N \\frac{L_{ij}}{d_j}p_j \\;\\;\\;\\;\\;\\; (1)\n",
    "$$\n",
    "\n",
    "Let's interpret what is happening in this equation. Inuitively, the component $\\sum_{j=1}^N L_{ij}p_j/d_j$ says that the importance $p_i$ of page $i$ is given by the weighted sum of the importance of pages which link to it, where the weights are $L_{ij}/d_j$. This ensures that each page can distribute a total weight of 1 to other pages, since \n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N \\frac{L_{ij}}{d_j} = \\frac{\\sum_{i=1}^n L_{ij}}{\\sum_{i=1}^N L_{ij}} = 1.\n",
    "$$\n",
    "The scalar $\\alpha$ (apparently set to 0.85 in the original implementation) is a weighting factor which governs how much influence the rank other pages have on the importance of page $i$. \n",
    "\n",
    "The PageRank algorithm describes an approach to find the ranking vector $\\boldsymbol{p}$ satisfying $(1)$, by reducing the problem to an eigenvalue problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63225aa",
   "metadata": {},
   "source": [
    "### Problem 1: exploiting matrix properties\n",
    "\n",
    "Note that we can write the relation (1) in matrix form as\n",
    "\n",
    "$$\n",
    "\\boldsymbol{p} = (1-\\alpha)\\boldsymbol{1} + \\alpha \\boldsymbol{L}\\boldsymbol{D}^{-1}\\boldsymbol{p}\n",
    "$$\n",
    "\n",
    "where we define $\\boldsymbol{D} = \\text{diag}(d_1,\\dots, d_N)$. Moreover, since we use the normalization $\\boldsymbol{1^\\top p} = N$, we can equivalently write this relation as \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{p} &= (1-\\alpha)\\boldsymbol{11^\\top p}/N + \\alpha \\boldsymbol{L}\\boldsymbol{D}^{-1}\\boldsymbol{p}\\\\\n",
    "&= [(1-\\alpha)\\boldsymbol{11}^\\top/N + \\alpha \\boldsymbol{L}\\boldsymbol{D}^{-1}] \\boldsymbol{p}\\\\\n",
    "&= \\boldsymbol{Ap}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where we defined the $N\\times N$ matrix $\\boldsymbol{A} = (1-\\alpha)\\boldsymbol{11}^\\top/N + \\alpha \\boldsymbol{L}\\boldsymbol{D}^{-1}$. \n",
    "\n",
    "Now we arrive at something that should look familiar: $\\boldsymbol{Ap} = \\boldsymbol{p}$. This says that the PageRank vector $\\boldsymbol{p}$ should be an _eigenvector_ of the matrix $\\boldsymbol{A}$ associated with the eigenvalue 1.\n",
    "\n",
    "It turns out we can actually prove that the matrix $\\boldsymbol{A}$ defined above always has an eigenvalue of 1. However, let's verify that this is true. Using the simple adjacency matrix $\\boldsymbol{L}$ defined at the beginning of the notebook, construct the matrix $\\boldsymbol{A}$ and find its eigenvalues. Verify that one of the eigenvalues is equal to 1, and find the eigenvector $\\boldsymbol{p}$ associated with this eigenvalue, and rescale the vector so that it sums to $N$ (i.e. multiply by $N/\\boldsymbol{1^\\top p}$). For the simple linked \"webpages\" described by the graph at the beginning of this notebook, which page is ranked the highest? Describe why the makes sense given what we can see visually about the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b9d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d5202f3",
   "metadata": {},
   "source": [
    "### Problem 2: the power method for large graphs\n",
    "\n",
    "When you we only have a few webpages, like in the toy graph presented earlier, computing a full eigenvalue decomposition is a completely reasonable way to solve the PageRank problem. However, realistic web searches will of course involve *many* more pages than this (for example, if you Google \"linear algebra\" you will find 150,000,000+ results). In this case, it is not feasible to compute a full eigenvalue decomposition for $\\boldsymbol{A}$ defined in Part 2. Instead, the PageRank algorithm uses something called the _power method_ to find the top eigenvector (which will always be associated with the eigenvalue 1) of the matrix $\\boldsymbol{A}$.\n",
    "\n",
    "To do this, we initialize $\\boldsymbol{p}^{(0)} = (1,\\dots, 1)$, and perform the following iterations for $t=1,\\dots, T$:\n",
    "\n",
    "$$\n",
    "\\tilde{\\boldsymbol{p}}^{(t+1)} = \\boldsymbol{A}\\boldsymbol{p}^{(t)},\\;\\;\\; \\boldsymbol{p}^{(t+1)} = N\\frac{\\tilde{\\boldsymbol{p}}}{\\boldsymbol{1}^\\top \\tilde{\\boldsymbol{p}}^{(t+1)}}\n",
    "$$\n",
    "\n",
    "Implement the power iterations for $T=10$ steps defined above for the following randomly generated adjacency matrix. Verify that the vector $\\boldsymbol{p}^{(T)}$ that you get at the end is an eigenvector of the matrix $\\boldsymbol{A}$ -- i.e. that $\\boldsymbol{A}\\boldsymbol{p}^{(T)}$ is just a rescaled version of $\\boldsymbol{p}^{(T)}$ (hint: to do this, you can check that $\\boldsymbol{Ap}/\\boldsymbol{p}$ is a constant vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdbe9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 1, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "N = 1000\n",
    "\n",
    "# randomly generate an N x N adjacency matrix\n",
    "L = np.random.binomial(1, 0.3, size=(N,N))\n",
    "\n",
    "# make sure all the diagonal entries are equal to zero\n",
    "np.fill_diagonal(L, 0)\n",
    "\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55a7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f06e81f5",
   "metadata": {},
   "source": [
    "## Part 2: Latent semantic indexing\n",
    "\n",
    "Suppose we had a corpus of $D$ documents (e.g. webpages) of text which we would like to group into \"topics\", but that we don't a priori know what these topics should be. This problem is generally known as _topic modeling_. Latent semantic indexing (LSI) (sometimes called latent semantic analysis (LSA)) is a simple and popular approach to topic modeling that relies on a term-document matrix associated with the collection of documents.\n",
    "\n",
    "In the context of text analysis, a _term_ typically refers to an individuals word. Let's say in our corpus of $D$ documents that there are $T$ distinct terms (and for the sake of this example, assume that $T > D$, so there are more words than documents). Then the term-document matrix is the matrix $\\boldsymbol{M}$ such that\n",
    "\n",
    "$$\n",
    "M_{ij} = \\text{ number of times term $i$ appears in document $j$}.\n",
    "$$\n",
    "\n",
    "Since we assume $T>D$, this will be a \"tall\" matrix. Note that row $i$, denoted $\\boldsymbol{t}_i$ and of length $n$, represents how word $i$ is distributed across the documents, while column $j$, denoted $\\boldsymbol{d}_j$ and of length $m$, represents the distribution of words present in document $j$. Correspondingly, $\\boldsymbol{t}_i^\\top \\boldsymbol{t}_{i'}$ represents a measure of similarity between terms $i$ and $i'$, in terms of how they appear in documents. Similarly for $\\boldsymbol{d}_j^\\top \\boldsymbol{d}_{j'}$ and the similarity between documents $j$ and $j'$, in terms of which terms appear in them. These two types of similarity can be calculated using the matrices \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\underbrace{\\boldsymbol{MM}^\\top}_{T\\times T} \\hspace{10mm} \\text{containing all the pairwise similarities between terms, and}\\\\\n",
    "&\\underbrace{\\boldsymbol{M^\\top M}}_{D\\times D} \\hspace{10mm} \\text{containing all the pairwise similarities between documents}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Similar to the covariance matrices in PCA, we can take an eigenvalue decomposition of either of these matrices, as they will be symmetric and positive semi-definite. Doing so is precisely what LSA entails, as we will see in the following problems.\n",
    "\n",
    "### Problem 1: extracting document-topic scores\n",
    "Let's focus on what this means for the matrix $\\boldsymbol{M^\\top M}$ of documents similarities. In this case, we get the eigenvalue decomposition\n",
    "\n",
    "$$\n",
    "\\boldsymbol{M^\\top M} = \\boldsymbol{V\\Lambda V^\\top}\n",
    "$$\n",
    "\n",
    "Like in PCA, the columns in $\\boldsymbol{V}$ represent the most informative \"directions\" along which various documents can be similar. We can interpret the top $k\\leq D$ of these directions as representing individual \"topics\". That is, if we extract the first $k$ vectors stored in the columns of $\\boldsymbol{V}$, we can get a $D\\times k$ _document-topic_ matrix $\\boldsymbol{V}_k$. For document $j$, the entries in the $j^{th}$ row of this matrix represent how much document $j$ aligns with each topic. \n",
    "\n",
    "In this problem, we will extract these scores for a very simple set of documents, defined in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fea358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"the person went to the store to buy an apple\",\n",
    "    \"the woman can buy a car at the store\",\n",
    "    \"you eat an apple every morning\",\n",
    "    \"the car wash is closed on sunday\",\n",
    "    \"i drive my car on sunday morning\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afe3901",
   "metadata": {},
   "source": [
    "The python package `scikit-learn` has a useful function which will allow us to extract a term-document matrix from this corpus. Note that $\\boldsymbol{M}$ here is $26\\times 5$, as our corpus contains 26 \"documents\" and 5 word; each row corresponds to a different word, each column to a different document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70e09ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0],\n",
       "       [1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [2, 2, 0, 1, 0],\n",
       "       [2, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "vectorizer = CountVectorizer()\n",
    "M = vectorizer.fit_transform(corpus).toarray().T\n",
    "print(M.shape) # 26 words, 5 documents\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31651895",
   "metadata": {},
   "source": [
    "Compute an eigenvalue decomposition of $\\boldsymbol{M^\\top M}$, and plot its eigenvalues from largest to smallest. Then, get the document-topic matrix $\\boldsymbol{V}_k$ (containing the top $k$ columns of $\\boldsymbol{V}$, correspoding to the largest $k$ eigenvalues). Do this for $k=3$, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4469822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954c127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2830daa6",
   "metadata": {},
   "source": [
    "### Problem 2: extracting the word-topic matrix\n",
    "\n",
    "Instead of finding which words are associated with which topic, we could instead try and find which words are most associated with each topic. To do this, we instead need an eigenvalue decomposition of the term similarity matrix: \n",
    "$$\n",
    "\\boldsymbol{MM^\\top} = \\boldsymbol{U\\Lambda' U^\\top}\n",
    "$$ \n",
    "\n",
    "Now $\\boldsymbol{U}$ is a $T\\times T$ matrix. If we select the top $k$ columns from this matrix, we can get scores that indicate the \"alignment\" of each word against each of the $k$ topics. Do the following:\n",
    "\n",
    "1. Compute the eigenvalue decomposition of $\\boldsymbol{MM^\\top}$, and verify that the eigenvalues of $\\boldsymbol{\\Lambda}'$ are closely related to those of $\\boldsymbol{\\Lambda}$ in problem 1.\n",
    "2. Construct the word-topic matrix $\\boldsymbol{U}_k$ by selecting the top $k=3$ columns of $\\boldsymbol{U}$ (associated with the $k$ largest eigenvalues). Which words are most associated with topic 2? Why does this makes sense in the document-topic scores you saw in problem 1?\n",
    "\n",
    "**An important remark:** Selecting $k$ topics using the topic matrix $\\boldsymbol{MM^\\top}$ or the document matrix $\\boldsymbol{M^\\top M}$ actually describe the same latent \"topics\". This is because the eigenvalue decompositions of these two matrices can be simplified into a single _singular value decomposition_, wherein $\\boldsymbol{M} = \\boldsymbol{U\\Sigma V}^\\top$ where $\\boldsymbol{U},\\boldsymbol{V}$ are as in the eigenvalues decompositions we discussed, and $\\boldsymbol{\\Sigma}$ is matrix containing the singular values of $\\boldsymbol{M}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a8f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9116e7605a3c7636cbb64cda962b2f0e89693208c0f8626394020c0fb858b688"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
