
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>2.7. Maximum likelihood estimation &#8212; Stat 151, Linear Models</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.8. Hypothesis testing for the Gaussian model" href="hypothesis_testing.html" />
    <link rel="prev" title="2.6. More on least squares" href="more_least_squares.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Stat 151, Linear Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../overview.html">
   Welcome to Stat 151
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../python_basics/chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chheader.html">
   2. Basics of linear regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="visualizing_data.html">
     2.1. Exploring and visualizing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="simple_linear_regression.html">
     2.2. Simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="simple_linear_regression_cont.html">
     2.3. More on simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="vectors_and_matrices.html">
     2.4. Basic concepts from linear algebra: vectors and matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="multiple_predictors.html">
     2.5. Linear regression with multiple predictors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="more_least_squares.html">
     2.6. More on least squares
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     2.7. Maximum likelihood estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="hypothesis_testing.html">
     2.8. Hypothesis testing for the Gaussian model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="diagnostics.html">
     2.9. Diagnostics for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../generalizing_linear_regression/chheader.html">
   3. Generalizing linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../generalizing_linear_regression/ridge_and_lasso.html">
     3.1. Regularizing regression: LASSO and Ridge
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../generalizing_linear_regression/nonlinear_regression.html">
     3.2. Fitting nonlinear functions: polynomial regression and kernel ridge regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../generalizing_linear_regression/logistic_regression.html">
     3.3. GLMs part 1: logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extensions_applications/chheader.html">
   4. Extensions and applications
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../../_sources/content/basic_linear_regression/maximum_likelihood_estimation.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/basic_linear_regression/maximum_likelihood_estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/basic_linear_regression/maximum_likelihood_estimation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review-of-the-general-theory-of-maximum-likelihood">
   2.7.1. Review of the general theory of maximum likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gaussian-model-for-linear-regression">
   2.7.2. The Gaussian model for linear regression
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Maximum likelihood estimation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#review-of-the-general-theory-of-maximum-likelihood">
   2.7.1. Review of the general theory of maximum likelihood
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-gaussian-model-for-linear-regression">
   2.7.2. The Gaussian model for linear regression
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="maximum-likelihood-estimation">
<h1><span class="section-number">2.7. </span>Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this headline">¶</a></h1>
<p>In this section, we will cover a basic review of <em>maximum likelihood estimation</em> – a general approach to fitting a statistical model to observed data. The goal will be to introduce the Gaussian model of linear regression, wherein we make the explicit assumption that the errors of the linear regression model follow a normal distribution. We will then review some basic tools for evaluating whether or not this assumption holds in practice. Later, we will use this model to build tools to perform statistical inference on the parameters of the linear regression model, such as performing hypothesis tests and creating confidence intervals for parameters.</p>
<div class="section" id="review-of-the-general-theory-of-maximum-likelihood">
<h2><span class="section-number">2.7.1. </span>Review of the general theory of maximum likelihood<a class="headerlink" href="#review-of-the-general-theory-of-maximum-likelihood" title="Permalink to this headline">¶</a></h2>
<p>Maximum likelihood estimation is a general framework for estimating the parameters of a statistical model. The approach works as follows: we observe data <span class="math notranslate nohighlight">\(z_1,\dots, z_n\)</span>, which we assume to be drawn independently and identically distributed from some distribution <span class="math notranslate nohighlight">\(p_{\boldsymbol{\theta}}(z)\)</span> which is parameterized by an unknown vector of parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta} \in \Theta\)</span>. For example, we might suppose that <span class="math notranslate nohighlight">\(z_1,\dots,z_n\)</span> all are drawn from a normal distribution:</p>
<div class="math notranslate nohighlight">
\[
z_1,\dots,z_n \sim N(\mu,\sigma^2)
\]</div>
<p>where in this case <span class="math notranslate nohighlight">\(\boldsymbol{\theta} = (\mu, \sigma^2)\)</span> are the parameters of the model. Our goal is to, given the observations <span class="math notranslate nohighlight">\(z_1,\dots, z_n\)</span>, estimate the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. To do this, we first specify the <em>likelihood</em> function, which is simply the joint probability of the sample <span class="math notranslate nohighlight">\(z_1,\dots,z_n\)</span>. Since these observations are assumed to be statistically independent, this joint probability is simply the product of the individual densities:</p>
<div class="math notranslate nohighlight">
\[
p_{\boldsymbol{\theta}}(z_1,\dots,z_n) = \prod_{i=1}^n p_{\boldsymbol{\theta}}(z_i).
\]</div>
<p>This gives a valid probability for any <em>fixed</em> value of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>. However, our goal here is to try and estimate the value of <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> that generated our data. To do this, we instead think of this joint distribution as a function of the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> (rather than a function of the data). When we do this, we refer to the joint distribution as the <em>likelihood</em> function:</p>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\theta}; z_1,\dots,z_n) = \prod_{i=1}^n p_{\boldsymbol{\theta}}(z_i).
\]</div>
<p>The principal of maximum likelihood estimation states that we should estimate <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> by choosing the value <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}\)</span> which makes the observed data the most likely, i.e. that maximizes <span class="math notranslate nohighlight">\(L(\boldsymbol{\theta}; z_1,\dots,z_n)\)</span>. For many statistical models, it will be more convenient to use the logarithm of the likelihood function to perform this maximization instead, or alternatively to minimize the negative log likelihood:</p>
<div class="math notranslate nohighlight">
\[
-\ell(\boldsymbol{\theta}; z_1,\dots, z_n) = -\log L(\boldsymbol{\theta}; z_1,\dots, z_n).
\]</div>
<p>(This will give the same result since <span class="math notranslate nohighlight">\(-\log\)</span> is a strictly decreasing function.)</p>
<p>Let’s work through an example using the normal distribution, both using math and python. To do this, suppose that we generate <span class="math notranslate nohighlight">\(n=100\)</span> samples independently from a true distribution of <span class="math notranslate nohighlight">\(N(-2, 4)\)</span>. In python, we can do this with the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">mu_true</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>
<span class="n">sigma2_true</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Note: the function np.random.normal takes scale to be</span>
<span class="c1"># the standard deviation, not the variance,</span>
<span class="c1"># so we need to take the square root</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_true</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-2.85652314 -1.14157277 -3.49662592 -0.48791666 -5.58189497  1.17901625
  0.43576949 -1.96188837  2.59561151 -2.45167457 -2.25840432 -3.12192587
  0.15840338 -5.77049617 -0.86825117 -1.26742066  0.54823504 -4.5823029
 -1.56597909  1.35138013 -3.34592012 -5.54259972 -5.46158745 -2.43587539
  1.77302328 -5.05725124 -7.11529312 -4.14346655  0.09543674 -6.27704147
 -4.01465268 -1.99971686 -2.98583311 -1.48223406 -2.28834488 -2.39819427
 -0.90278638 -4.25661202 -1.91873531 -1.92298829 -3.48349684 -4.36632812
 -1.86966808 -1.84458685 -1.16241232 -1.72273866 -4.07638458 -3.38858554
 -3.33302166 -2.66786688  0.37359225 -2.55614399 -2.31945748 -3.13740966
  0.61317941 -1.6976306  -2.77532047 -1.8601692  -1.5713832  -4.67168636
 -0.98164854 -0.63413678 -2.55082726 -2.18503633 -3.53242172 -2.36610764
 -0.83291886 -2.73581488 -2.64340728 -1.97172756 -4.16737597 -2.78444831
 -2.09532068 -2.23636106 -3.57634167 -1.3713749  -2.76299926 -2.00967274
 -1.11748471  2.09747504 -1.40913336 -1.11230733 -3.41777568 -0.82228924
 -2.58605864  1.17021127 -2.26599607 -2.12603507 -3.11349578 -4.65856785
 -4.90818995 -2.53748984 -5.17055792 -0.72518064  1.90539018  0.30653722
 -4.67848337 -2.36788964 -4.82514918 -1.1117415 ]
</pre></div>
</div>
</div>
</div>
<p>Now let’s pretend we don’t know the values of <span class="math notranslate nohighlight">\((\mu_{true}, \sigma^2_{true})\)</span>, and try and estimate them using maximum likelihood estimation. To do this, we first need to write down the likelihood function. For a single observation <span class="math notranslate nohighlight">\(z_i\)</span>, the density of the <span class="math notranslate nohighlight">\(N(\mu,\sigma^2)\)</span> distribution is given by</p>
<div class="math notranslate nohighlight">
\[
p_{\mu,\sigma^2}(z) = \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(z-\mu)^2}{2\sigma^2}}.
\]</div>
<p>The the likelihood function is just the product of <span class="math notranslate nohighlight">\(n\)</span> of these:</p>
<div class="math notranslate nohighlight">
\[
L(\mu,\sigma^2;z_1,\dots,z_n) = \prod_{i=1}^n p_{\mu,\sigma^2}(z_i) = (2\pi \sigma^2)^{-n/2} e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n (z_i-\mu)^2}.
\]</div>
<p>Now we will use the trick where we try and minimize the negative log likelihood:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
-\ell(\mu,\sigma^2; z_1,\dots,z_n) &amp;= -\log L(\mu,\sigma^2;z_1,\dots,z_n) \\
&amp;= \frac{n}{2}\log(2\pi \sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (z_i - \mu)^2.
\end{align*}
\end{split}\]</div>
<p>To find the minimum of this function (corresponding to the maximum of the likelihood function), we need to take derivatives with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span> and set them equal to zero. First we do this for <span class="math notranslate nohighlight">\(\mu\)</span> to get <span class="math notranslate nohighlight">\(\hat{\mu}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{d}{d\mu}-\ell(\mu,\sigma^2; z_1,\dots,z_n) &amp;= -\frac{1}{\sigma^2}\sum_{i=1}^n (z_i - \mu) = 0 \\
&amp;\implies \hat{\mu} = \frac{1}{n}\sum_{i=1}^n z_i = \bar{z}.
\end{align*}
\end{split}\]</div>
<p>Next, for <span class="math notranslate nohighlight">\(\sigma^2\)</span> we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{d}{d\sigma^2}-\ell(\mu,\sigma^2; z_1,\dots,z_n) &amp;= \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4}\sum_{i=1}^n (z_i-\mu)^2 = 0 \\
&amp;\implies \hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (z_i-\hat{\mu})^2 = \frac{1}{n}\sum_{i=1}^n (z_i-\bar{z})^2.
\end{align*}
\end{split}\]</div>
<p>These estimators are called the <em>maximum likelihood estimators</em> of <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Let’s calculate them for the data we sampled above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mle_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="n">mle_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">z</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">mle_mean</span><span class="p">,</span> <span class="n">mle_variance</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(-2.2525480600860974, 3.6595530135511787)
</pre></div>
</div>
</div>
</div>
<p>As we can see, these already do a reasonably good job of estimating our true parameters <span class="math notranslate nohighlight">\(\mu_{true}=-2,\sigma^2_{true}=4\)</span>.</p>
<p>We might expect that these estimates get better as we gather more samples. Indeed, this is one important property of maximum likelihood estimators: they are asymptotically <em>consistent</em>, meaning that, if the true data is actually generated from the model distribution, then</p>
<div class="math notranslate nohighlight">
\[
\lim_{n\to \infty} \hat{\boldsymbol{\theta}} = \boldsymbol{\theta}_{true}.
\]</div>
<p>We can simulate this to check that it is indeed the case for the normal distribution example given here. We will do this with the following simulation: for each <span class="math notranslate nohighlight">\(n = 5, 10, 20, 40,\cdots, 1280\)</span> we will sample <span class="math notranslate nohighlight">\(B=50\)</span> datasets of size <span class="math notranslate nohighlight">\(n\)</span>, and compute the maximum likelihood estimates of the mean and variance. We will plot this against the ground truth to estimate how quickly these converge.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># create a figure for side-by-side plots</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># specify number of trials and range of n values we&#39;ll try</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_range</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="o">*</span><span class="nb">int</span><span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)]</span>

<span class="n">mmeans</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># use this to store the mean of the means</span>
<span class="n">smeans</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># use this to store the sd of the means</span>

<span class="n">mvars</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># means of the variances</span>
<span class="n">svars</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># standard deviation of the variance</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">n_range</span><span class="p">:</span>
    <span class="n">curr_means</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># to store the results at the current step</span>
    <span class="n">curr_vars</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2_true</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="c1"># generate samples</span>
        <span class="n">curr_means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">))</span> <span class="c1"># compute MLE mean</span>
        <span class="n">curr_vars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">z</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">z</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># compute MLE variance</span>

    <span class="n">mmeans</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">curr_means</span><span class="p">))</span> <span class="c1"># mean of the means</span>
    <span class="n">smeans</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">curr_means</span><span class="p">))</span> <span class="c1"># standard dev of means</span>
    <span class="n">mvars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">curr_vars</span><span class="p">))</span> <span class="c1"># mean of the variances</span>
    <span class="n">svars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">curr_vars</span><span class="p">))</span> <span class="c1"># standard dev of the variances</span>

<span class="c1"># convert these from lists to numpy arrays, makes calculations easier below</span>
<span class="n">mmeans</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mmeans</span><span class="p">)</span>
<span class="n">smeans</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">smeans</span><span class="p">)</span>
<span class="n">mvars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">mvars</span><span class="p">)</span>
<span class="n">svars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">svars</span><span class="p">)</span>

<span class="c1"># plot the results for the means, with shaded area represented 1 standard dev</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_range</span><span class="p">,</span> <span class="n">mmeans</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;MLE mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">n_range</span><span class="p">,</span> <span class="n">mmeans</span><span class="o">-</span><span class="n">smeans</span><span class="p">,</span> <span class="n">mmeans</span><span class="o">+</span><span class="n">smeans</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">mu_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">n_range</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">n_range</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Estimated mean&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MLE mean vs n&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># plot the results for the variances, with shaded area represented 1 standard dev</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_range</span><span class="p">,</span> <span class="n">mvars</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;MLE variance&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">n_range</span><span class="p">,</span> <span class="n">mvars</span><span class="o">-</span><span class="n">svars</span><span class="p">,</span> <span class="n">mvars</span><span class="o">+</span><span class="n">svars</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">sigma2_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">n_range</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">n_range</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True variance&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Estimated variance&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;MLE variance vs n&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/maximum_likelihood_estimation_5_0.png" src="../../_images/maximum_likelihood_estimation_5_0.png" />
</div>
</div>
<p>From the plot we can see that the MLE estimates quickly become very accurate estimates of the true parameter values. Of course, in this setting, we simulated our data, so we <em>know</em> that the normal distribution was the correct model. When this is not the case (as it typically is not in practice), we will need to develop tools to be able to assess our distributional assumptions. Later in this section, we will discuss one such tool.</p>
</div>
<div class="section" id="the-gaussian-model-for-linear-regression">
<h2><span class="section-number">2.7.2. </span>The Gaussian model for linear regression<a class="headerlink" href="#the-gaussian-model-for-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>Now that we’ve reviewed the basics of maximum likelihood estimation, we can apply this approach to our linear model. Let’s first write down our basic linear model:</p>
<div class="math notranslate nohighlight">
\[
y_i = \boldsymbol{\beta}\cdot \boldsymbol{x}_i + \varepsilon_i,
\]</div>
<p>where so far we have assumed that <span class="math notranslate nohighlight">\(\varepsilon_i\)</span> satisfies:</p>
<ul class="simple">
<li><p>independence across observations <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p>zero mean equal variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> across observations</p></li>
</ul>
<p>To be able to perform maximum likelihood estimation on the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> of our linear model, we will need to add a bit more than this. In particular, we will assume that the errors follow a normal distribution (also called the Gaussian distribution), so that</p>
<div class="math notranslate nohighlight">
\[
\varepsilon_1,\dots,\varepsilon_n \stackrel{i.i.d.}{\sim} N(0,\sigma^2).
\]</div>
<p>Moreover, we will continue to assume that the <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>’s are known and not random. Then this means that the response <span class="math notranslate nohighlight">\(y_i\)</span> is modeled to be the sum of a non-random term <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\cdot \boldsymbol{x}_i\)</span>, plus a normally distributed random variable <span class="math notranslate nohighlight">\(\varepsilon_i \sim N(0,\sigma^2)\)</span>. Hence under this model, <span class="math notranslate nohighlight">\(y_i \sim N(\boldsymbol{\beta}\cdot \boldsymbol{x}_i, \sigma^2)\)</span>.</p>
<p>Now we can write down the joint likelihood of <span class="math notranslate nohighlight">\(y_1,\dots,y_n\)</span> conditioned on <span class="math notranslate nohighlight">\(\boldsymbol{x}_1,\dots,\boldsymbol{x}_n\)</span> <em>and</em> the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta},\sigma^2\)</span>. (Note that <span class="math notranslate nohighlight">\(\sigma^2\)</span> is also a parameter than we need to estimate in this model!)</p>
<div class="math notranslate nohighlight">
\[
L(\boldsymbol{\beta},\sigma^2\mid (\boldsymbol{x}_i,y_i)_{i=1}^n)=p(y_1,\dots,y_n\mid \boldsymbol{x}_1,\dots,\boldsymbol{x}_n,\boldsymbol{\beta},\sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}e^{-\frac{(y_i - \boldsymbol{\beta}\cdot \boldsymbol{x}_i)}{2\sigma^2}}
\]</div>
<p>The negative log-likelihood becomes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
-\ell(\boldsymbol{\beta},\sigma^2\mid (\boldsymbol{x}_i,y_i)_{i=1}^n) &amp;= -\log L(\boldsymbol{\beta},\sigma^2\mid (\boldsymbol{x}_i,y_i)_{i=1}^n)\\
&amp;= \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \boldsymbol{\beta}\cdot \boldsymbol{x}_i)^2 \\
&amp;= \frac{n}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\|\boldsymbol{y} - \boldsymbol{X\beta}\|_2^2,
\end{align*}
\end{split}\]</div>
<p>where here we’ve defined <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> to be the matrix containing <span class="math notranslate nohighlight">\(\boldsymbol{x}_1,\dots,\boldsymbol{x}_n\)</span> as its rows, and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> to be the vector containing <span class="math notranslate nohighlight">\(y_1,\dots,y_n\)</span>. Notice that the term that depends on <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is exactly our least squares objective <span class="math notranslate nohighlight">\(\|\boldsymbol{y} - \boldsymbol{X\beta}\|_2^2\)</span>! This means if we take derivatives with respect to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and set them equal to zero, we will get exactly our usual solution back:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} = (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top y}.
\]</div>
<p>This means that our usual least squares estimates for the parameters of the linear regression model <em>are the maximum likelihood estimates of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> under the Gaussian model</em>. Previously, we chose the least-squares problem as a reasonable heuristic to use to fit a linear regression, but now maximum likelihood gives us a principled statistical model that naturally yields the same estimator.</p>
<p>For <span class="math notranslate nohighlight">\(\sigma^2\)</span>, the derivation works essentially the same as in our simple normal MLE problem above:</p>
<div class="math notranslate nohighlight">
\[
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (y_i - \boldsymbol{\beta}\cdot \boldsymbol{x}_i)^2 = \frac{1}{n}\|\boldsymbol{y}-\hat{\boldsymbol{y}}\|_2^2.
\]</div>
<blockquote>
<div><p>Remark: we will often work with a different estimator of the variance, namely <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = \frac{1}{n-p}\|\boldsymbol{y}-\hat{\boldsymbol{y}}\|_2^2\)</span>. The reason for the scaling <span class="math notranslate nohighlight">\(1/(n-p)\)</span> is that it makes this version of <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span> an <em>unbiased</em> estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span>, in the sense that <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\sigma}^2] = \sigma^2\)</span>.</p>
</div></blockquote>
<p>While none of this is particularly new – we obtain the same quantities we got before ever explicitly introducing the Gaussian model – we can now do some additional cool things. For example, we can actually derive the distribution of the parameters <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>. To do this, note that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is a linear function of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, hence</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\hat{\boldsymbol{\beta}} &amp;= (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top y}\\
&amp;= (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top (\underbrace{\boldsymbol{X\beta}_\star + \boldsymbol{\varepsilon}}_{(i)})}\\
&amp;= (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top X \beta}_\star + (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top\varepsilon}\\
&amp;= \boldsymbol{\beta}_\star + (\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top\varepsilon}
\end{align*}
\end{split}\]</div>
<p>Here <span class="math notranslate nohighlight">\((i)\)</span> holds because we are assuming that there is some true underlying linear model, i.e. that <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{X\beta}_\star + \boldsymbol{\varepsilon}\)</span> for some unknown parameters <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_\star\)</span>. Under this assumption, we get a linear combination of two terms: the first is <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_\star\)</span>, or the “true” regression parameters, which we assume have generated our data. The second is <span class="math notranslate nohighlight">\((\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top\varepsilon}\)</span>, which is really just a linear transformation of the Gaussian random vector <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}\)</span>, which itself will follow a normal distribution. Let’s figure out what its parameters are. Since <span class="math notranslate nohighlight">\(\mathbb{E}[\boldsymbol{\varepsilon}] = 0\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[(\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top\varepsilon}] = 0.
\]</div>
<p>To compute the variance, we need the following fact: if <span class="math notranslate nohighlight">\(\boldsymbol{Z} \sim N(0,\sigma^2 \boldsymbol{I})\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is a matrix, then <span class="math notranslate nohighlight">\(\boldsymbol{AZ} \sim N(0, \sigma^2\boldsymbol{AA^\top})\)</span>.</p>
<p>Hence, the variance of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\text{Cov}[(\boldsymbol{X^\top X})^{-1} \boldsymbol{X^\top\varepsilon}] =  (\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top}\sigma^2 \boldsymbol{I}\boldsymbol{X}(\boldsymbol{X^\top X})^{-1}= \sigma^2 (\boldsymbol{X^\top X})^{-1}.
\]</div>
<p>Thus we have finally that</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{\beta}_\star, \sigma^2 (\boldsymbol{X^\top X})^{-1}).
\]</div>
<p>From this we can get a few things: first, we know that <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> is an <em>unbiased</em> estimator of the true regression coefficients, meaning that <span class="math notranslate nohighlight">\(\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}_\star\)</span>. Second, we can derive the marginal distribution of each individual coefficient: <span class="math notranslate nohighlight">\(\beta_j \sim N(\beta_{\star,j}, \sigma^2 (\boldsymbol{X^\top X})^{-1}_{jj})\)</span> where <span class="math notranslate nohighlight">\((\boldsymbol{X^\top X})^{-1}_{jj}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>th diagonal entry of the matrix <span class="math notranslate nohighlight">\((\boldsymbol{X^\top X})^{-1}\)</span>. This can be used, for example, to construct confidence intervals for the values of the coefficients. Note that all the variances here depend on <span class="math notranslate nohighlight">\(\sigma^2\)</span>, which is unknown, so in practice we will simply plug in the estimated value <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>, and sometimes switch to using the <span class="math notranslate nohighlight">\(t\)</span>-distribution to account for the fact that we must also estimate the variance; we will talk more about this in another section.</p>
<blockquote>
<div><p>Remark: when we say “distribution of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>” we need to be clear what randomness we’re talking about. Specifically, we mean randomness that comes from the response <span class="math notranslate nohighlight">\(y_i\)</span> <em>conditioned on</em> <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>. So if we’re being careful about notation, we should really write <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\mid \boldsymbol{X} \sim N(\boldsymbol{\beta}_\star, \sigma^2 (\boldsymbol{X^\top X})^{-1})\)</span> to indicate that the <em>conditional distribution</em> of <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span> <em>given</em> <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> follows this normal distribution.</p>
</div></blockquote>
<p>Let’s work through a simple example of obtaining the full distribution of the regression coefficients. For this, we will use the fish toxicity dataset, which attempts to predict the toxicity of certain drugs for a type of fish using various molecular properties of the drugs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/qsar_fish_toxicity.csv&quot;</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CIC0</th>
      <th>SM1_Dz(Z)</th>
      <th>GATS1i</th>
      <th>NdsCH</th>
      <th>NdssC</th>
      <th>MLOGP</th>
      <th>LC50</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.260</td>
      <td>0.829</td>
      <td>1.676</td>
      <td>0</td>
      <td>1</td>
      <td>1.453</td>
      <td>3.770</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.189</td>
      <td>0.580</td>
      <td>0.863</td>
      <td>0</td>
      <td>0</td>
      <td>1.348</td>
      <td>3.115</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.125</td>
      <td>0.638</td>
      <td>0.831</td>
      <td>0</td>
      <td>0</td>
      <td>1.348</td>
      <td>3.531</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.027</td>
      <td>0.331</td>
      <td>1.472</td>
      <td>1</td>
      <td>0</td>
      <td>1.807</td>
      <td>3.510</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.094</td>
      <td>0.827</td>
      <td>0.860</td>
      <td>0</td>
      <td>0</td>
      <td>1.886</td>
      <td>5.390</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>As usual, we will extract the response as a numpy array, and the other as another array, adding a column of 1’s to account for the intercept term in the regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># extract the data</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;LC50&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="n">c</span><span class="o">!=</span><span class="s2">&quot;LC50&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">cols</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="c1"># add a column of ones to the X matrix</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">ones</span><span class="p">,</span> <span class="n">X</span><span class="p">])</span>

<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((908, 7), (908,))
</pre></div>
</div>
</div>
</div>
<p>Next, we need to fit the regression model to get the coefficients <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}\)</span>, and also get the estimated value <span class="math notranslate nohighlight">\(\hat{\sigma}^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute beta_hat = (XTX)^{-1}XTy</span>
<span class="n">XTX</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">XTX_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">XTX</span><span class="p">)</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">XTX_inv</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># compute sigma2_hat = |y-y_hat|^2/n</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_hat</span><span class="p">)</span>
<span class="n">sigma2_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">y_hat</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># print the coefficients and the estimated variance for each coefficient</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;intercept&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">cols</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cols</span><span class="p">)):</span>
    <span class="n">sigma2_hat_j</span> <span class="o">=</span> <span class="n">sigma2_hat</span><span class="o">*</span><span class="n">XTX_inv</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;coefficent for </span><span class="si">{</span><span class="n">cols</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">beta_hat</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">, variance for </span><span class="si">{</span><span class="n">cols</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">sigma2_hat_j</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>coefficent for intercept: 2.1746, variance for intercept: 0.0326
coefficent for CIC0 : 0.3856, variance for CIC0 : 0.0037
coefficent for SM1_Dz(Z) : 1.2556, variance for SM1_Dz(Z) : 0.0075
coefficent for GATS1i : -0.7464, variance for GATS1i : 0.0102
coefficent for NdsCH: 0.4136, variance for NdsCH: 0.0029
coefficent for NdssC: 0.0643, variance for NdssC: 0.0016
coefficent for MLOGP: 0.3901, variance for MLOGP: 0.0011
</pre></div>
</div>
</div>
</div>
<p>In this section, we covered the basics of maximum likelihood estimation, and saw that if we assume a Gaussian model for the errors in linear regression, use the ML framework to obtain the same least-squares estimates for the parameters as we had before. In the coming sections, we will 1) develop diagnostic tools for assessing the plausibility of the Gaussian model and 2) use the Gaussian model to perform statistical inference, such as hypothesis tests and confidence intervals for the coefficients.</p>
<!--
## Checking the assumptions of the Gaussian model using QQ plots
In the above, we found that we could get a full distribution of the coefficients $\hat{\boldsymbol{\beta}}$ for the Gaussian model of linear regression. In practice, however, if we'd like to use these results, we need to verify that this model is actually reasonable for the data we've obtained. Recall that this model critically relied on two assumptions:

1. The model is linear, so $y_i = \boldsymbol{\beta}_\star \cdot \boldsymbol{x}_i + \varepsilon_i$ for some coefficients $\boldsymbol{\beta}_\star$
2. The errors $\varepsilon_i$ are drawn i.i.d. from a normal distribution $N(0,\sigma^2)$.

In practice, it's unlikely that either of these are _exactly_ true. However, they may be close enough to being true to yield meaningful results. (See the [famous quote](https://en.wikipedia.org/wiki/All_models_are_wrong) by the statistical George Box.) We will discuss more about the assumption of linearity later; in this section, we will focus on checking whether or not the errors appear to follow a normal distribution. --></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/basic_linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="more_least_squares.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2.6. </span>More on least squares</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="hypothesis_testing.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2.8. </span>Hypothesis testing for the Gaussian model</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>