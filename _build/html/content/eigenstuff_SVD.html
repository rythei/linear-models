
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.5. The Singular Value Decomposition &#8212; Linear Algebra for Data Workbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.6. Low-rank approximation using the SVD" href="eigenstuff_low-rank-approximation.html" />
    <link rel="prev" title="7.4. The QR algorithm for finding eigenvalues and eigenvectors" href="eigenstuff_qr-algorithm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Linear Algebra for Data Workbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Welcome to Stat 89A
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="python_chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_1_chheader.html">
   2. Matrices, vectors, and
   <span class="math notranslate nohighlight">
    \(\mathbb{R}^n\)
   </span>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_introduction-to-norms.html">
     2.1. Introduction to Norms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_norms-integration-monte-carlo.html">
     2.2. An application: approximating integrals with norms and Monte Carlo integration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_lp-balls.html">
     2.3.
     <span class="math notranslate nohighlight">
      \(\ell_p\)
     </span>
     Balls
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_classification-with-norms.html">
     2.4. An application: classifying data points using norms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_2_chheader.html">
   3. Basics of vectors and vector spaces
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_2A_vectorspaces.html">
     3.1. Vectors and vector spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3_chheader.html">
   4. Basics of matrices
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices-and-matrix-operations.html">
     4.1. Matrices and matrix operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_deconstructing.html">
     4.2. Deconstructing Matrix Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_powers_of_matrices.html">
     4.3. Taking Powers of Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3B_chheader.html">
   5. Matrices as transformations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_linear_examples_new.html">
     5.1. Linear and Nonlinear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices_and_linear_functions.html">
     5.2. Matrices and Linear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_injective-and-surjective-functions.html">
     5.3. Injective, surjective and invertible functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_inverses.html">
     5.4. Left Inverses, Right Inverses, and Inverses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_changing_basis.html">
     5.5. Changing Basis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_4_chheader.html">
   6. Geometry: angles, orthogonality, and projections
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_dot-products-and-angles.html">
     6.1. Dot products, angles, and orthogonality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR.html">
     6.2. Gram–Schmidt and the QR Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR_linear_systems.html">
     6.3. Solving linear systems with the QR decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_projections.html">
     6.4. Projections
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The EVD, SVD and applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="eigenstuff_chheader.html">
   7. Eigenthings
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_quadratic-forms.html">
     7.1. Quadratic forms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings.html">
     7.2. Eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings-special-matrices.html">
     7.3. The Eigenvalue decomposition for special types of matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_qr-algorithm.html">
     7.4. The QR algorithm for finding eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.5. The Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_low-rank-approximation.html">
     7.6. Low-rank approximation using the SVD
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="applications_chheader.html">
   8. Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_PCA.html">
     8.1. Principal Component Analysis (PCA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_spectral-clustering.html">
     8.2. Sprectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_least-squares.html">
     8.3. Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_double-descent.html">
     8.4. The “double descent” phenomenon
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/content/eigenstuff_SVD.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/eigenstuff_SVD.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/eigenstuff_SVD.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-the-svd-in-python">
   7.5.1. Computing the SVD in Python
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The Singular Value Decomposition</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-the-svd-in-python">
   7.5.1. Computing the SVD in Python
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-singular-value-decomposition">
<h1><span class="section-number">7.5. </span>The Singular Value Decomposition<a class="headerlink" href="#the-singular-value-decomposition" title="Permalink to this headline">¶</a></h1>
<p>In the previous workbook, we saw that symmetric square matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> have a special decomposition called an <em>eigenvalue</em> decomposition: <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{V\Lambda V}^\top\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> is an orthogonal matrix satisfying <span class="math notranslate nohighlight">\(\boldsymbol{V^\top V} = \boldsymbol{VV}^\top = \boldsymbol{I}\)</span>, whose columns are <em>eigenvectors</em>, and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \text{diag}(\lambda_1,\dots, \lambda_n)\)</span> is a diagonal matrix containing the <em>eigenvalues</em> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>.</p>
<p>In this section, we see that <em>all</em> matrices – even non-square and non-symmetric – have a similar decomposition called the <em>singular value decomposition</em>, or SVD. Let’s first remind ourselves why we can’t eigenvalue decomposition doesn’t make sense for non-square matrices. Suppose <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is a <span class="math notranslate nohighlight">\(m\times n\)</span> matrix. Then for any <span class="math notranslate nohighlight">\(\boldsymbol{v}\in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{Av}\)</span> is a vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>, and so the eigenvalue condition <span class="math notranslate nohighlight">\(\boldsymbol{Av} = \lambda \boldsymbol{v}\)</span> does not make sense in this setting: the left-hand side is a <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector, while <span class="math notranslate nohighlight">\(\lambda \boldsymbol{v}\)</span> is an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector.</p>
<p>Instead, for <span class="math notranslate nohighlight">\(m\times n\)</span> matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, we consider instead a generalized version of the eigenvalue condition: vectors <span class="math notranslate nohighlight">\(\boldsymbol{v}\in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{u}\in \mathbb{R}^m\)</span> and a number <span class="math notranslate nohighlight">\(\sigma\)</span> are called <em>right and left singular vectors</em>, and a <em>singular value</em> if they satisfy:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{Av} = \sigma \boldsymbol{u} &amp;&amp; (1)\\
\boldsymbol{A^\top u} = \sigma \boldsymbol{v} &amp;&amp; (2)
\end{aligned}
\end{split}\]</div>
<p>Singular values and singular vectors are in fact closely related to eigenvalues and eigenvectors. Let’s see why this is the case. Let’s start with equation <span class="math notranslate nohighlight">\((1)\)</span>, and multiply both sides by <span class="math notranslate nohighlight">\(\boldsymbol{A}^\top\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{Av} = \sigma \boldsymbol{u} \implies \boldsymbol{A^\top A v} = \sigma \boldsymbol{A^\top u}.
\]</div>
<p>Now, let’s plug in equation <span class="math notranslate nohighlight">\((2)\)</span>, which says that <span class="math notranslate nohighlight">\(\boldsymbol{A^\top u} = \sigma \boldsymbol{v}\)</span>. We get:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A^\top A v} = \sigma \boldsymbol{A^\top u} = \sigma^2 \boldsymbol{v}.
\]</div>
<p>This looks more like something we’ve seen before: if we set <span class="math notranslate nohighlight">\(\boldsymbol{B} = \boldsymbol{A^\top A}\)</span> and <span class="math notranslate nohighlight">\(\lambda = \sigma^2\)</span>, this can be written as <span class="math notranslate nohighlight">\(\boldsymbol{Bv} = \lambda \boldsymbol{v}\)</span>. Therefore, the squared singular values and right singular vectors can be obtained by computing an eigenvalue decompostion of the symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span>. Using a similar derivation, we can also show that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{AA^\top u} = \sigma^2 \boldsymbol{u}
\]</div>
<p>from which we see that <span class="math notranslate nohighlight">\(u\)</span> is really an eigenvector of the symmetric matrix <span class="math notranslate nohighlight">\(AA^\top\)</span>.</p>
<p><strong>Remark:</strong> In our discussion above, we saw that the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> and/or <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> correspond to the <em>squared</em> singular values <span class="math notranslate nohighlight">\(\sigma^2\)</span> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. This may seem odd, since we know that in general matrices may have positive or negative eigenvalues. However, this occurs specifically because <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> are always <em>positive semi-definite</em>, and therefore always have non-negative eigenvalues. To see why this is true, note that the smallest eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> are the minimum of the quadratic form <span class="math notranslate nohighlight">\(Q(\boldsymbol{x}) = \boldsymbol{x^\top A^\top A x}\)</span>, over all unit vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[
\lambda_{\text{min}} = \min_{\|\boldsymbol{x}\|_2 =1} \boldsymbol{x^\top A^\top A x} = \min_{\|\boldsymbol{x}\|_2 =1} (\boldsymbol{Ax})^\top \boldsymbol{Ax} = \min_{\|\boldsymbol{x}\|_2 =1}\|\boldsymbol{Ax}\|_2^2 \geq 0.
\]</div>
<p>A similar derivation shows that all the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> are non-negative.</p>
<p>How many singular values/vectors do we expect to get for a given <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>? We know that the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> is <span class="math notranslate nohighlight">\(n\times n\)</span>, which gives us <span class="math notranslate nohighlight">\(n\)</span> eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots, \boldsymbol{v}_n\)</span> (corresponding to <span class="math notranslate nohighlight">\(n\)</span> right singular vectors of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>), and <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> is <span class="math notranslate nohighlight">\(m\times m\)</span>, giving us <span class="math notranslate nohighlight">\(m\)</span> eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_1,\dots, \boldsymbol{u}_m\)</span> (corresponding to <span class="math notranslate nohighlight">\(m\)</span> left singular vectors of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>). The matrices <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> will of course not have the same number of eigenvalues, though they do always have the same <em>non-zero</em> eigenvalues. The number <span class="math notranslate nohighlight">\(r\)</span> of nonzero eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> and/or <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span>  is exactly equal to the <em>rank</em> of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, and we always have that <span class="math notranslate nohighlight">\(r \leq \min(m,n)\)</span>.</p>
<p>Now let’s collect the vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_1,\dots, \boldsymbol{u}_m\)</span> into an <span class="math notranslate nohighlight">\(m\times m\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{U} = \begin{bmatrix} \boldsymbol{u}_1 &amp; \cdots &amp; \boldsymbol{u}_m\end{bmatrix}\)</span> and likewise with <span class="math notranslate nohighlight">\(\boldsymbol{v}_1,\dots, \boldsymbol{v}_n\)</span> into the <span class="math notranslate nohighlight">\(n\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{V} = \begin{bmatrix}\boldsymbol{v}_1 &amp;\cdots &amp; \boldsymbol{v}_n\end{bmatrix}\)</span>. Note that since <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> come from the eigenvalue decompositions of the symmetric matrices <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span>, we have that <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> are always orthogonal, satisfying <span class="math notranslate nohighlight">\(\boldsymbol{U^\top U} = \boldsymbol{UU}^\top = \boldsymbol{I}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V^\top V} = \boldsymbol{VV}^\top = \boldsymbol{I}\)</span>.</p>
<p>Then let’s define the <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}_{ij} = \begin{cases}\sigma_i &amp; \text{if } i=j\\ 0  &amp; \text{if } i\neq j\end{cases}
\end{split}\]</div>
<p>That is, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a “rectangular diagonal” matrix, whose diagonal entries are the singular values of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> – i.e. the square roots of the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span>. For example, in the <span class="math notranslate nohighlight">\(2\times 3\)</span> case <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> would generically look like</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}\sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_2 &amp;0\end{bmatrix}
\end{split}\]</div>
<p>and in the <span class="math notranslate nohighlight">\(3\times 2\)</span> case it would look like</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}\sigma_1 &amp; 0  \\ 0 &amp; \sigma_2 \\ 0 &amp; 0\end{bmatrix}.
\end{split}\]</div>
<p>Given the matrices <span class="math notranslate nohighlight">\(\boldsymbol{U}, \boldsymbol{\Sigma}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span>, we can finally write the full singular value decomposition of <span class="math notranslate nohighlight">\(A\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A} = \boldsymbol{U\Sigma V}^\top.
\]</div>
<p>This is one of the most important decompositions in linear algebra, especially as it relates to statistics, machine learning and data science.</p>
<p><strong>Remark:</strong> Sometimes you may see a slightly different form of the SVD: the rank of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is <span class="math notranslate nohighlight">\(r\leq \min(n,m)\)</span>, we can actually remove the last <span class="math notranslate nohighlight">\(m-r\)</span> columns of <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> and <span class="math notranslate nohighlight">\(n-r\)</span> column of <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> (so that <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is <span class="math notranslate nohighlight">\(m\times r\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> is <span class="math notranslate nohighlight">\(n\times r\)</span>), and let <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> be the <span class="math notranslate nohighlight">\(r\times r\)</span> diagonal matrix <span class="math notranslate nohighlight">\(\text{diag}(\sigma_1,\dots,\sigma_r)\)</span>. The two forms are totally equivalent, since the last <span class="math notranslate nohighlight">\(m-r\)</span> columns of <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> are only multiplied by the <span class="math notranslate nohighlight">\(m-r\)</span> zero rows at the bottom of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> anyway. This form is sometimes called the “compact SVD”. In this workbook, we’ll assume we’re working with the “standard” version, introduced above, though the compact version is sometimes better to work with in practice, especially when the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is very low rank, with <span class="math notranslate nohighlight">\(r\ll m,n\)</span>.</p>
<div class="section" id="computing-the-svd-in-python">
<h2><span class="section-number">7.5.1. </span>Computing the SVD in Python<a class="headerlink" href="#computing-the-svd-in-python" title="Permalink to this headline">¶</a></h2>
<p>Let’s see some examples of computing the singular value decomposition in Python.</p>
<p>First, let’s draw a random <span class="math notranslate nohighlight">\(m\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> to use.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Next, let’s compute the eigenvalue decompositions of <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A} = \boldsymbol{V\Lambda}_1 \boldsymbol{V}^\top\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{AA^\top} = \boldsymbol{U\Lambda}_2 \boldsymbol{U}^\top\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">AAT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">ATA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">A</span><span class="p">)</span>

<span class="n">Lambda1</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">ATA</span><span class="p">)</span>
<span class="n">Lambda2</span><span class="p">,</span> <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">AAT</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Of course, since <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> are of different dimensions, <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_2\)</span> will also be of different dimensions. However, as we mentioned above, <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_1\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_2\)</span> should have the same <em>non-zero</em> entries. Let’s check that this is true.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">Lambda1</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Lambda2</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[15.67530452  6.09646829  0.42380744]
[15.67530452  6.09646829  0.42380744 -0.          0.        ]
</pre></div>
</div>
</div>
</div>
<p>Indeed, we get the same non-zero eigenvalues, but <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda}_2\)</span> has 10 extra zero eigenvalues. Now let’s form the matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, which will be <span class="math notranslate nohighlight">\(m\times n\)</span> matrix with <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{ii} = \sqrt{\lambda_i}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{ij} = 0\)</span> for <span class="math notranslate nohighlight">\(i\neq j\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">Sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Lambda1</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">Sigma</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[3.95920504, 0.        , 0.        ],
       [0.        , 2.46910273, 0.        ],
       [0.        , 0.        , 0.65100495],
       [0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        ]])
</pre></div>
</div>
</div>
</div>
<p>Now we have our matrices <span class="math notranslate nohighlight">\(\boldsymbol{V},\boldsymbol{U}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>; let’s check that <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{U\Sigma V}^\top\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>Strangely, this doesn’t give us the correct answer. The reason is that we have an issue with one of the signs of the eigenvectors: the eigenvalue is invariant to switching the signs of one of the eigenvectors (i.e. multiplying one of the columns of <span class="math notranslate nohighlight">\(V\)</span> or <span class="math notranslate nohighlight">\(U\)</span> by <span class="math notranslate nohighlight">\(-1\)</span> ), but the SVD is not. Since we computed the eigenvalue decomposition of <span class="math notranslate nohighlight">\(\boldsymbol{A^\top A}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{AA}^\top\)</span> separately, there was no guarantee that we would get the correct signs of the eigenvectors. It turns out in this case we can fix this by switching the sign of the third column of <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span> <span class="o">*=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Now everything works! However, this issue is a bit annoying in practice – fortunately, we can avoid it by simply using numpy’s build in SVD function, <code class="docutils literal notranslate"><span class="pre">np.linalg.svd</span></code>. Let’s see how this works.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">VT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="n">n</span><span class="p">))</span> <span class="c1"># make diagonal matrix</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">Sigma</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">VT</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="eigenstuff_qr-algorithm.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7.4. </span>The QR algorithm for finding eigenvalues and eigenvectors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="eigenstuff_low-rank-approximation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.6. </span>Low-rank approximation using the SVD</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney, N. Benjamin Erichson and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>