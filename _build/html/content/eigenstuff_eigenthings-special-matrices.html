
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>7.3. The Eigenvalue decomposition for special types of matrices &#8212; Linear Algebra for Data Workbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.4. The QR algorithm for finding eigenvalues and eigenvectors" href="eigenstuff_qr-algorithm.html" />
    <link rel="prev" title="7.2. Eigenvalues and eigenvectors" href="eigenstuff_eigenthings.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Linear Algebra for Data Workbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Welcome to Stat 89A
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="python_chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_1_chheader.html">
   2. Matrices, vectors, and
   <span class="math notranslate nohighlight">
    \(\mathbb{R}^n\)
   </span>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_introduction-to-norms.html">
     2.1. Introduction to Norms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_norms-integration-monte-carlo.html">
     2.2. An application: approximating integrals with norms and Monte Carlo integration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_lp-balls.html">
     2.3.
     <span class="math notranslate nohighlight">
      \(\ell_p\)
     </span>
     Balls
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_classification-with-norms.html">
     2.4. An application: classifying data points using norms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_2_chheader.html">
   3. Basics of vectors and vector spaces
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_2A_vectorspaces.html">
     3.1. Vectors and vector spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3_chheader.html">
   4. Basics of matrices
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices-and-matrix-operations.html">
     4.1. Matrices and matrix operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_deconstructing.html">
     4.2. Deconstructing Matrix Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_powers_of_matrices.html">
     4.3. Taking Powers of Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3B_chheader.html">
   5. Matrices as transformations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_linear_examples_new.html">
     5.1. Linear and Nonlinear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices_and_linear_functions.html">
     5.2. Matrices and Linear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_injective-and-surjective-functions.html">
     5.3. Injective, surjective and invertible functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_inverses.html">
     5.4. Left Inverses, Right Inverses, and Inverses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_changing_basis.html">
     5.5. Changing Basis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_4_chheader.html">
   6. Geometry: angles, orthogonality, and projections
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_dot-products-and-angles.html">
     6.1. Dot products, angles, and orthogonality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR.html">
     6.2. Gram–Schmidt and the QR Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR_linear_systems.html">
     6.3. Solving linear systems with the QR decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_projections.html">
     6.4. Projections
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The EVD, SVD and applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="eigenstuff_chheader.html">
   7. Eigenthings
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_quadratic-forms.html">
     7.1. Quadratic forms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings.html">
     7.2. Eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     7.3. The Eigenvalue decomposition for special types of matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_qr-algorithm.html">
     7.4. The QR algorithm for finding eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_SVD.html">
     7.5. The Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_low-rank-approximation.html">
     7.6. Low-rank approximation using the SVD
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="applications_chheader.html">
   8. Applications
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_PCA.html">
     8.1. Principal Component Analysis (PCA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_spectral-clustering.html">
     8.2. Sprectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_least-squares.html">
     8.3. Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_double-descent.html">
     8.4. The “double descent” phenomenon
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/content/eigenstuff_eigenthings-special-matrices.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/eigenstuff_eigenthings-special-matrices.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/eigenstuff_eigenthings-special-matrices.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#symmetric-matrices">
   7.3.1. Symmetric matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projection-matrices">
   7.3.2. Projection matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#triangular-matrices">
   7.3.3. Triangular matrices
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The Eigenvalue decomposition for special types of matrices</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#symmetric-matrices">
   7.3.1. Symmetric matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#projection-matrices">
   7.3.2. Projection matrices
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#triangular-matrices">
   7.3.3. Triangular matrices
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-eigenvalue-decomposition-for-special-types-of-matrices">
<h1><span class="section-number">7.3. </span>The Eigenvalue decomposition for special types of matrices<a class="headerlink" href="#the-eigenvalue-decomposition-for-special-types-of-matrices" title="Permalink to this headline">¶</a></h1>
<p>In the previous section, we saw that the general eigenvalue problem for an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> involves finding <span class="math notranslate nohighlight">\(n\)</span> value vector pairs <span class="math notranslate nohighlight">\((\lambda_1, \boldsymbol{u}_1),\dots, (\lambda_n, \boldsymbol{u}_n)\)</span> satisfying</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{Au}_i = \lambda_i \boldsymbol{u}_i.
\]</div>
<p>where we find the values <span class="math notranslate nohighlight">\(\lambda_i\)</span> by finding the roots of the <em>characteristic polynomial</em> <span class="math notranslate nohighlight">\(p(\lambda) = \det(\boldsymbol{A} - \lambda \boldsymbol{I})\)</span>, and we find the associated eigenvectors by solving the linear system <span class="math notranslate nohighlight">\((\boldsymbol{A}-\lambda \boldsymbol{I})\boldsymbol{u} = 0\)</span>.</p>
<p>If we form the eigenvector into the columns of an <span class="math notranslate nohighlight">\(n\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{U} = \begin{bmatrix} \boldsymbol{u_1} &amp; \dots &amp; \boldsymbol{u}_n \end{bmatrix}\)</span> and the eigenvalues into a diagonal matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Lambda} = \text{diag}(\lambda_1,\dots,\lambda_n)\)</span>, then this can equivalently be expressed as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{AU} = \boldsymbol{U\Lambda} \implies \boldsymbol{A} = \boldsymbol{U\Lambda U}^{-1} \,\,\,\, \text{(assuming $\boldsymbol{U}$ invertible)}
\]</div>
<p>This form is typically called the <em>eigenvalue decomposition</em> of the matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. In this section, we will see special cases of this decomposition resulting from special types of matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. The most important of these is the case of symmetric matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, which we discuss next.</p>
<div class="section" id="symmetric-matrices">
<h2><span class="section-number">7.3.1. </span>Symmetric matrices<a class="headerlink" href="#symmetric-matrices" title="Permalink to this headline">¶</a></h2>
<p>In general, the eigenvalues and eigenvectors of a matrix may be real or complex valued, as they come from the roots of polynomial. For example, in the <span class="math notranslate nohighlight">\(2 \times 2\)</span> case, with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{A} = \begin{bmatrix} a_{11}&amp; a_{12}\\ a_{21} &amp; a_{22}\end{bmatrix}
\end{split}\]</div>
<p>the characteristic polynomial will be <span class="math notranslate nohighlight">\(p(\lambda) = \lambda^2 - (a_{11}+a_{22})\lambda + a_{11}a_{22}-a_{12}a_{21}\)</span>, which, by the quadratic formula, will have complex roots (and hence complex eigenvalues) whenever <span class="math notranslate nohighlight">\((a_{11}+a_{22})^2 - 4(a_{11}a_{22}-a_{12}a_{21}) &lt; 0\)</span>. On the other hand, if <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is <em>symmetric</em>, so that <span class="math notranslate nohighlight">\(\boldsymbol{A}^\top = \boldsymbol{A}\)</span>, then the eigenvalues and eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> will always satisfy the following two properties:</p>
<ul class="simple">
<li><p>The eigenvalues of a symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> are always real numbers.</p></li>
<li><p>The eigenvectors of a symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> are always orthogonal.</p></li>
</ul>
<p>In particular, the latter condition means that the matrix <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> of eigenvectors is an <em>orthogonal matrix</em> satisfying <span class="math notranslate nohighlight">\(\boldsymbol{U^\top U} = \boldsymbol{I}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{UU^\top} = \boldsymbol{I}\)</span> (since <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is square). This means that <span class="math notranslate nohighlight">\(\boldsymbol{U}^{-1} = \boldsymbol{U}^\top\)</span>, and so in the symmetric case we can simplify the eigenvalue decomposition:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{A} = \boldsymbol{U\Lambda U}^\top.
\]</div>
<p>Here we briefly provide proofs for both of the above statements about the eigenvalues and eigenvectors of symmetric matrices <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>.</p>
<p><strong>Eigenvalues are real numbers.</strong> Let’s quickly see why symmetric matrices always have real-valued eigenvalues. For a number/vector <span class="math notranslate nohighlight">\(x\)</span>, let <span class="math notranslate nohighlight">\(\bar{x}\)</span> denote its complex conjugate (for a vector, this is just the complex conjugate in each coordinate). Then for any real matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, let <span class="math notranslate nohighlight">\((\lambda, \boldsymbol{v})\)</span> be an eigenvalue/eigenvector pair of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>. Since <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is real-valued, we have that <span class="math notranslate nohighlight">\(\overline{\boldsymbol{Av}} = \bar{\lambda}\bar{\boldsymbol{v}}\)</span>. Then if <span class="math notranslate nohighlight">\(\boldsymbol{A}=\boldsymbol{A}^\top\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\bar{\boldsymbol{v}}^\top \boldsymbol{A} \boldsymbol{v} = \bar{\boldsymbol{v}}^\top (\lambda \boldsymbol{v}) = \lambda \bar{\boldsymbol{v}}^\top \boldsymbol{v}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\bar{\boldsymbol{v}}^\top \boldsymbol{A v} = \bar{\boldsymbol{v}}^\top \boldsymbol{A^\top v} = (\boldsymbol{A}\bar{\boldsymbol{v}}^\top)\boldsymbol{v} = \bar{\lambda}\bar{\boldsymbol{v}}^\top \boldsymbol{v}.
\]</div>
<p>Therefore <span class="math notranslate nohighlight">\(\lambda \bar{\boldsymbol{v}}^\top \boldsymbol{v} = \bar{\lambda}\bar{\boldsymbol{v}}^\top \boldsymbol{v}\)</span> which implies <span class="math notranslate nohighlight">\(\lambda = \bar{\lambda}\)</span>, and so <span class="math notranslate nohighlight">\(\lambda\)</span> must be a real number.</p>
<p><strong>Eigenvectors are orthogonal.</strong> Next, let’s see why the eigenvectors of symmetric matrices are orthogonal. Suppose <span class="math notranslate nohighlight">\((\lambda, \boldsymbol{u})\)</span> and <span class="math notranslate nohighlight">\((\mu, \boldsymbol{v})\)</span> are two eigenvalue/eigenvector pairs for a symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> such that <span class="math notranslate nohighlight">\(\lambda \neq \mu\)</span> (i.e. they are distinct eigenvalues). Then <span class="math notranslate nohighlight">\(\boldsymbol{Au} = \lambda \boldsymbol{u}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{Av} = \mu \boldsymbol{v}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\lambda \boldsymbol{u^\top v} &amp;= (\lambda \boldsymbol{u})^\top \boldsymbol{v} = (\boldsymbol{Au})^\top \boldsymbol{v}\\
&amp;= \boldsymbol{u}^\top \underbrace{\boldsymbol{A}^\top}_{=\boldsymbol{A}} \boldsymbol{v}= \boldsymbol{u}^\top (\boldsymbol{Av})\\ &amp;= \boldsymbol{u}^\top (\mu \boldsymbol{v}) = \mu \boldsymbol{u^\top v}
\end{align*}
\end{split}\]</div>
<p>Thus, rearranging we get</p>
<div class="math notranslate nohighlight">
\[
(\lambda - \mu)\boldsymbol{u^\top v} = 0 \implies \boldsymbol{u^\top v} = 0
\]</div>
<p>since <span class="math notranslate nohighlight">\((\lambda - \mu) \neq 0\)</span>, because by assumption the eigenvectors are orthogonal.</p>
<p>Let’s see an example in Python, using the <code class="docutils literal notranslate"><span class="pre">np.linalg.eig(A)</span></code> function to find eigenvalues. To find a symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, we will use the following approach: first, draw a random <span class="math notranslate nohighlight">\(n\times n\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{B}\)</span>, and then let <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{B^\top B}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span> <span class="c1"># B^T B is always a symmetric matrix</span>

<span class="n">Lambda</span><span class="p">,</span> <span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">Lambda</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">Lambda</span><span class="p">)</span> <span class="c1"># numpy returns Lambda as an array, so let&#39;s make it a diagonal matrix</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s verify that <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{U\Lambda U^\top}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ULUT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Lambda</span><span class="p">,</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">ULUT</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Indeed, the two matrices are the same. Next, let’s check that <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is in fact orthogonal, by checking that <span class="math notranslate nohighlight">\(\boldsymbol{U^\top U} = \boldsymbol{I}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">UTU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">U</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">U</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="n">UTU</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 1.,  0., -0., -0., -0.,  0., -0.,  0., -0.,  0.],
       [ 0.,  1., -0.,  0.,  0., -0.,  0.,  0., -0.,  0.],
       [-0., -0.,  1., -0., -0.,  0., -0.,  0., -0.,  0.],
       [-0.,  0., -0.,  1., -0., -0.,  0., -0., -0., -0.],
       [-0.,  0., -0., -0.,  1.,  0., -0., -0., -0., -0.],
       [ 0., -0.,  0., -0.,  0.,  1.,  0.,  0., -0.,  0.],
       [-0.,  0., -0.,  0., -0.,  0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  0., -0., -0.,  0.,  0.,  1.,  0.,  0.],
       [-0., -0., -0., -0., -0., -0.,  0.,  0.,  1., -0.],
       [ 0.,  0.,  0., -0., -0.,  0.,  0.,  0., -0.,  1.]])
</pre></div>
</div>
</div>
</div>
<p>We get the identity back, and so <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> is in fact orthogonal.</p>
</div>
<div class="section" id="projection-matrices">
<h2><span class="section-number">7.3.2. </span>Projection matrices<a class="headerlink" href="#projection-matrices" title="Permalink to this headline">¶</a></h2>
<p>Recall that a projection matrix <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> is a square matrix for which <span class="math notranslate nohighlight">\(\boldsymbol{P}^2 = \boldsymbol{P}\)</span>. Let’s see how the eigenvalues work out for such a matrix. If <span class="math notranslate nohighlight">\(\lambda, \boldsymbol{u}\)</span> is an eigenvalue/eigenvector pair for <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> then we have</p>
<div class="math notranslate nohighlight">
\[
\lambda \boldsymbol{u} = \boldsymbol{Pu} = \boldsymbol{P}^2\boldsymbol{u} = \boldsymbol{P}(\lambda \boldsymbol{u}) = \lambda^2 \boldsymbol{u}
\]</div>
<p>Hence <span class="math notranslate nohighlight">\((\lambda - \lambda^2)\boldsymbol{u} = 0\)</span>, and since the eigenvector <span class="math notranslate nohighlight">\(\boldsymbol{u}\)</span> is not equal to zero, we have <span class="math notranslate nohighlight">\(\lambda(1 - \lambda) = 0\)</span>, which implies <span class="math notranslate nohighlight">\(\lambda\)</span> is 0 zero or 1. Moreover, it turns out that the number of eigenvalues with value 1 is exactly equal to the <em>dimension</em> of the subspace that <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> projects onto. Let’s see why this works in the case of an orthogonal projection. Suppose that <span class="math notranslate nohighlight">\(\boldsymbol{P}\)</span> projects onto a subspace <span class="math notranslate nohighlight">\(V = \text{Range}(\boldsymbol{P})\)</span> of dimension <span class="math notranslate nohighlight">\(k\)</span>, and let <span class="math notranslate nohighlight">\(q_1,\dots,q_k\)</span> be an orthonormal basis for this subspace. Then define <span class="math notranslate nohighlight">\(\boldsymbol{Q} = \begin{bmatrix} q_1 &amp; \dots &amp; q_k\end{bmatrix}\)</span>, and note that we have that <span class="math notranslate nohighlight">\(\boldsymbol{P} = \boldsymbol{QQ}^\top\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{Q^\top Q} = \boldsymbol{I}_k\)</span>. To see the next step, we first need the following fact</p>
<blockquote>
<div><p>Let <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> be a symmetric <span class="math notranslate nohighlight">\(n\times n\)</span> matrix with real eigenvalues <span class="math notranslate nohighlight">\(\lambda_{1},\dots,\lambda_n\)</span>. Then <span class="math notranslate nohighlight">\(\text{trace}(\boldsymbol{A}) = \sum_{i=1}^n a_{i,i} = \sum_{i=1}^n \lambda_i\)</span>. That is, the sum of the diagonal entries of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span> is equal to the sum of the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>.</p>
</div></blockquote>
<p>With this fact in hand, we observe that</p>
<div class="math notranslate nohighlight">
\[
\# \{\lambda_i = 1\} = \sum_{i=1}^n \lambda_i = \text{trace}(\boldsymbol{P}) = \text{trace}(\boldsymbol{QQ}^\top) = \text{trace}(\boldsymbol{Q^\top Q}) = \text{trace}(\boldsymbol{I}_k) = k.
\]</div>
<p>Hence any orthogonal projection matrix onto a subspace of dimension <span class="math notranslate nohighlight">\(k\)</span> will have exactly <span class="math notranslate nohighlight">\(k\)</span> eigenvalues equal to <span class="math notranslate nohighlight">\(1\)</span>, and <span class="math notranslate nohighlight">\(n-k\)</span> eigenvalues equal to <span class="math notranslate nohighlight">\(0\)</span>. We will see an example of this in Python after introducing the next type of special matrix.</p>
</div>
<div class="section" id="triangular-matrices">
<h2><span class="section-number">7.3.3. </span>Triangular matrices<a class="headerlink" href="#triangular-matrices" title="Permalink to this headline">¶</a></h2>
<p>Another import example is triangular matrices. Consider an <span class="math notranslate nohighlight">\(n\times n\)</span> (upper) triangular matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R} = \begin{bmatrix} r_{1,1} &amp; r_{1,2} &amp; \cdots &amp; r_{1,n}\\ 0 &amp; r_{2,2} &amp; \cdots &amp; r_{2,n}\\ \vdots &amp; \vdots &amp;\ddots &amp; \vdots\\ 0 &amp; 0 &amp; \cdots &amp; r_{n,n}  \end{bmatrix}.
\end{split}\]</div>
<p>Let’s find the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>. Recall that one way to characterize the eigenvalues of a matrix is by finding all values of <span class="math notranslate nohighlight">\(\lambda\)</span> for which <span class="math notranslate nohighlight">\((\boldsymbol{R} - \lambda \boldsymbol{I})\)</span> is <em>not</em> invertible. We claim that for triangular matrices, this occurs exactly when <span class="math notranslate nohighlight">\(\lambda\)</span> is equal to one of the diagonal entries of <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>. Let’s see what happens when we set <span class="math notranslate nohighlight">\(\lambda = r_{1,1}\)</span>. Then we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R} - \lambda \boldsymbol{I} = \boldsymbol{R} - r_{1,1} \boldsymbol{I} = \begin{bmatrix} 0 &amp; r_{1,2} &amp; \cdots &amp; r_{1,n}\\ 0 &amp; r_{2,2}-r_{1,1} &amp; \cdots &amp; r_{2,n}\\ \vdots &amp; \vdots &amp;\ddots &amp; \vdots\\ 0 &amp; 0 &amp; \cdots &amp; r_{n,n} - r_{1,1} \end{bmatrix}
\end{split}\]</div>
<p>Now clearly since the first column is equal to zero, the columns of <span class="math notranslate nohighlight">\(\boldsymbol{R} - \lambda \boldsymbol{I}\)</span> are linearly dependent and hence the matrix is not invertible. Therefore <span class="math notranslate nohighlight">\(r_{1,1}\)</span> is an eigenvalue of <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span>. Similarly, if we take <span class="math notranslate nohighlight">\(\lambda = r_{2,2}\)</span>, then we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{R} - \lambda \boldsymbol{I} = \boldsymbol{R} - r_{2,2} \boldsymbol{I}= \begin{bmatrix} r_{1,1}-r_{2,2} &amp; r_{1,2} &amp; \cdots &amp; r_{1,n}\\ 0 &amp; 0 &amp; \cdots &amp; r_{2,n}\\ \vdots &amp; \vdots &amp;\ddots &amp; \vdots\\ 0 &amp; 0 &amp; \cdots &amp; r_{n,n}-r_{2,2}  \end{bmatrix}.
\end{split}\]</div>
<p>Now the columns are again linearly dependent, since <span class="math notranslate nohighlight">\(r_{1,1}-r_{2,2}\)</span> will be some scalar multiple of <span class="math notranslate nohighlight">\(r_{1,2}\)</span>, and so <span class="math notranslate nohighlight">\(r_{2,2}\)</span>. It’s easy to see that this would continue for all <span class="math notranslate nohighlight">\(n\)</span> diagonal entries, and therefore that the <span class="math notranslate nohighlight">\(n\)</span> eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> are exactly equal to its diagonal entries, i.e. <span class="math notranslate nohighlight">\(\lambda_{1} = r_{1,1}, \lambda_2 = r_{2,2},\dots, \lambda_n = r_{n,n}\)</span>.</p>
<p>Now let’s see an example in Python which illustrates both of the previous concepts. To do this, let’s first draw a random <span class="math notranslate nohighlight">\(n\times k\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>, and compute it’s QR decomposition <span class="math notranslate nohighlight">\(\boldsymbol{A} = \boldsymbol{QR}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span><span class="p">,</span><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">k</span><span class="p">))</span>
<span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now note that the matrix <span class="math notranslate nohighlight">\(\boldsymbol{P} = \boldsymbol{QQ^\top}\)</span> will be a projection matrix onto a subspace of dimension <span class="math notranslate nohighlight">\(k\)</span> (spanned by the <span class="math notranslate nohighlight">\(k\)</span> columns of <span class="math notranslate nohighlight">\(\boldsymbol{A}\)</span>), and <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is upper triangular. Let’s first compute the eigenvalue decomposition of the projection <span class="math notranslate nohighlight">\(\boldsymbol{QQ^\top}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">values_P</span><span class="p">,</span> <span class="n">vectors_P</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the sum of the the eigenvalues of P is &#39;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">values_P</span><span class="p">),</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the sum of the the eigenvalues of P is  5.0
</pre></div>
</div>
</div>
</div>
<p>Indeed, it is equal to <span class="math notranslate nohighlight">\(k=5\)</span>! Next, let’s check inspect the diagonal entries of the upper triangular matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> that we got from the QR decomposition.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the diagonal entries of R are &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">R</span><span class="p">))</span>
<span class="n">values_R</span><span class="p">,</span> <span class="n">vectors_R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;the eigenvalues of R are &#39;</span><span class="p">,</span> <span class="n">values_R</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the diagonal entries of R are  [ 3.94317512  2.65648448 -2.02191166 -2.7560563  -2.77326378]
the eigenvalues of R are  [ 3.94317512  2.65648448 -2.02191166 -2.7560563  -2.77326378]
</pre></div>
</div>
</div>
</div>
<p>Indeed, the eigenvalues of the upper triangular matrix <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> are exactly equal to its diagonal entries.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="eigenstuff_eigenthings.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">7.2. </span>Eigenvalues and eigenvectors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="eigenstuff_qr-algorithm.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7.4. </span>The QR algorithm for finding eigenvalues and eigenvectors</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney, N. Benjamin Erichson and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>