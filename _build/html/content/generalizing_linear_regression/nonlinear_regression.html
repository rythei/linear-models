
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3.2. Fitting nonlinear functions: polynomial regression and kernel ridge regression &#8212; Stat 151, Linear Models</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet">
  <link href="../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="4. Extensions and applications" href="../extensions_applications/chheader.html" />
    <link rel="prev" title="3.1. Regularizing regression: LASSO and Ridge" href="ridge_and_lasso.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Stat 151, Linear Models</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../overview.html">
   Welcome to Stat 151
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Linear models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../python_basics/chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../python_basics/python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../basic_linear_regression/chheader.html">
   2. Basics of linear regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/visualizing_data.html">
     2.1. Exploring and visualizing data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/simple_linear_regression.html">
     2.2. Simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/simple_linear_regression_cont.html">
     2.3. More on simple linear regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/vectors_and_matrices.html">
     2.4. Basic concepts from linear algebra: vectors and matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/multiple_predictors.html">
     2.5. Linear regression with multiple predictors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/more_least_squares.html">
     2.6. More on least squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/maximum_likelihood_estimation.html">
     2.7. Maximum likelihood estimation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/hypothesis_testing.html">
     2.8. Hypothesis testing for the Gaussian model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../basic_linear_regression/diagnostics.html">
     2.9. Diagnostics for linear regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="chheader.html">
   3. Generalizing linear regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="ridge_and_lasso.html">
     3.1. Regularizing regression: LASSO and Ridge
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     3.2. Fitting nonlinear functions: polynomial regression and kernel ridge regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../extensions_applications/chheader.html">
   4. Extensions and applications
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../../_sources/content/generalizing_linear_regression/nonlinear_regression.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/content/generalizing_linear_regression/nonlinear_regression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/generalizing_linear_regression/nonlinear_regression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-polynomials-using-least-squares">
   3.2.1. Fitting polynomials using least squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-ridge-regression">
   3.2.2. Kernel ridge regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-choosing-kernels">
   3.2.3. Appendix: choosing kernels
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Fitting nonlinear functions: polynomial regression and kernel ridge regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fitting-polynomials-using-least-squares">
   3.2.1. Fitting polynomials using least squares
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-ridge-regression">
   3.2.2. Kernel ridge regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-choosing-kernels">
   3.2.3. Appendix: choosing kernels
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="fitting-nonlinear-functions-polynomial-regression-and-kernel-ridge-regression">
<h1><span class="section-number">3.2. </span>Fitting nonlinear functions: polynomial regression and kernel ridge regression<a class="headerlink" href="#fitting-nonlinear-functions-polynomial-regression-and-kernel-ridge-regression" title="Permalink to this headline">¶</a></h1>
<div class="section" id="fitting-polynomials-using-least-squares">
<h2><span class="section-number">3.2.1. </span>Fitting polynomials using least squares<a class="headerlink" href="#fitting-polynomials-using-least-squares" title="Permalink to this headline">¶</a></h2>
<p>Thus far in this course, we have used the term linear regression to mean “fitting a line to data”. In this section, we will see that linear regression can actually be used as a general method to fit functions which are <em>nonlinear</em> functions of the data. In particular, in this section, we will focus on the task of fitting polynomials to data.</p>
<p>To see how this is done, suppose we would like to fit a <span class="math notranslate nohighlight">\(p\)</span>th order polynomial of the form</p>
<div class="math notranslate nohighlight">
\[
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p \hspace{10mm} (1)
\]</div>
<p>to some observed data <span class="math notranslate nohighlight">\((x, y)\)</span>. Then define the vectors</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{z}_p(x) = \begin{bmatrix} 1 \\ x \\ x^2 \\\vdots \\ x^p\end{bmatrix}\in\mathbb{R}^{p+1},\;\;\; \boldsymbol{\beta}=\begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \\\vdots \\ \beta_p\end{bmatrix} \in \mathbb{R}^{p+1}
\end{split}\]</div>
<p>and observe that the model <span class="math notranslate nohighlight">\((1)\)</span> can be equivalently expressed as</p>
<div class="math notranslate nohighlight">
\[
f(x) = \boldsymbol{z}_p(x)\cdot \boldsymbol{\beta}.
\]</div>
<p>Given some data <span class="math notranslate nohighlight">\((x_1,y_1),\dots,(x_n,y_n)\)</span> that we’d like to fit this model, we could define the usual least squares objective as</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n (y_i - \boldsymbol{z}_p(x_i)\cdot \boldsymbol{\beta})^2 = \|\boldsymbol{y} - \boldsymbol{X}_p \boldsymbol{\beta}\|_2^2
\]</div>
<p>where we’ve defined the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X}_p = \begin{bmatrix}\boldsymbol{z}_p(x_1)\\ \vdots \\ \boldsymbol{z}_p(x_1)\end{bmatrix} = \begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \cdots &amp; x_1^p\\ \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots \\ \vdots &amp; \vdots &amp; \vdots &amp;\vdots &amp; \vdots \\ 1 &amp; x_n &amp; x_n^2 &amp; \cdots &amp; x_n^p\end{bmatrix}
\end{split}\]</div>
<p>Our problem now looks identical to our usual least squares problem, and we can get the usual solution: <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}} = (\boldsymbol{X}_p^\top \boldsymbol{X}_p)^{-1}\boldsymbol{X}_p^\top \boldsymbol{y}\)</span>. Indeed, in this model, each power <span class="math notranslate nohighlight">\(x^j\)</span> just acts like an additional feature in our model. These coefficients can be plugged back in to get a fitted <span class="math notranslate nohighlight">\(p\)</span>th order polynomial:</p>
<div class="math notranslate nohighlight">
\[
\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 + \cdots + \hat{\beta}_p x^p.
\]</div>
<p>This is distinctly <em>not</em> a linear function of the data. The important thing for making the usual least squares problem appear is that it is a linear function of the <em>parameters</em> (i.e. the function <span class="math notranslate nohighlight">\(f_{\boldsymbol{\beta}}(x) = \boldsymbol{z}_p(x)\cdot \boldsymbol{\beta}\)</span> is a linear function of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span>). Let’s see an example to illustrate how this works. First, let’s generate some fake data coming from the “true” regression function <span class="math notranslate nohighlight">\(f_\star(x) = 4x + 3\cos(2\pi x)\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is drawn uniformly at random from <span class="math notranslate nohighlight">\([-1,1]\)</span> and</p>
<div class="math notranslate nohighlight">
\[
y = f_\star(x) + \varepsilon.
\]</div>
<p>We do this in the following cell.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">f_star</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">4</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f_\star$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_1_0.png" src="../../_images/nonlinear_regression_1_0.png" />
</div>
</div>
<p>To generate the polynomial feature matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}_p\)</span>, we define the following function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">x</span><span class="o">**</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s try fitting a polynomial to our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># use an order 5 polynomial </span>
<span class="n">Xp</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span> <span class="c1"># generate feature matrix</span>
<span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@Xp</span><span class="p">)</span><span class="nd">@Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1"># get OLS coefficients</span>

<span class="c1"># make predictions on test samples to compare the fitted function</span>
<span class="n">Xp_test</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">y_hat_test</span> <span class="o">=</span> <span class="n">Xp_test</span><span class="nd">@beta_hat</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f_\star$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_hat_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fitted degree 5 polynomial&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_5_0.png" src="../../_images/nonlinear_regression_5_0.png" />
</div>
</div>
<p>As we can see, our fitted model does a pretty reasonable job of approximating the true regression function <span class="math notranslate nohighlight">\(f_\star\)</span>. Of course, in this example, we simply picked <span class="math notranslate nohighlight">\(p=5\)</span> as the degree of the polynomial somewhat arbitrarily. Let’s see what happens when we try different values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">p_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> 

<span class="n">mses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">p_range</span><span class="p">:</span>
    <span class="n">Xp</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span> <span class="c1"># generate feature matrix</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@Xp</span><span class="p">)</span><span class="nd">@Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1"># get OLS coefficients</span>

    <span class="c1"># make predictions on test samples to compare the fitted function</span>
    <span class="n">Xp_test</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y_hat_test</span> <span class="o">=</span> <span class="n">Xp_test</span><span class="nd">@beta_hat</span>
    <span class="n">mses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_hat_test</span> <span class="o">-</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">20</span><span class="p">]:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_hat_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;p=</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f_\star$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_range</span><span class="p">,</span> <span class="n">mses</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_7_0.png" src="../../_images/nonlinear_regression_7_0.png" />
</div>
</div>
<p>On the left plot, we see examples of fitted polynomials of different degrees. As you might expect, the models get more complicated as the degree <span class="math notranslate nohighlight">\(p\)</span> gets larger. In the plot on the right, we plot the error (MSE) between the fitted values <span class="math notranslate nohighlight">\(\hat{f}_p(x)\)</span> of the degree <span class="math notranslate nohighlight">\(p\)</span> model against the true function values <span class="math notranslate nohighlight">\(f_\star(x)\)</span>. We see that the performance first gets better as we increase <span class="math notranslate nohighlight">\(p\)</span>, as the model becomes sufficiently expressive to fit the data, but that performance degrades for too large of <span class="math notranslate nohighlight">\(p\)</span> as the models become too complex and overfit. For this example, the best value of <span class="math notranslate nohighlight">\(p\)</span> appears to be <span class="math notranslate nohighlight">\(p=7\)</span> or <span class="math notranslate nohighlight">\(p=8\)</span>. Of course, in practice we do not have access to <span class="math notranslate nohighlight">\(f_\star\)</span>, and so we couldn’t use this method to pick <span class="math notranslate nohighlight">\(p\)</span>. Instead, we could use a method like cross validation to pick the best value. However, shortly we will see a different approach: we could choose <span class="math notranslate nohighlight">\(p\)</span> to be very large, and then use <em>regularization</em> to find a simpler model.</p>
<p>Let’s consider the situation when <span class="math notranslate nohighlight">\(p\)</span> is fixed to be an integer larger than <span class="math notranslate nohighlight">\(n\)</span>. In this case, the inverse <span class="math notranslate nohighlight">\((\boldsymbol{X^\top X})^{-1}\)</span> does not exist, and so we need to use something other than the standard least squares solution. A natural choice would be the Ridge regression solution, which is given by</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}_{RR}(\lambda) = (\boldsymbol{X}^\top \boldsymbol{X} + \lambda \boldsymbol{I})^{-1}\boldsymbol{X^\top y}.
\]</div>
<p>As we saw in the previous section, when we increase <span class="math notranslate nohighlight">\(\lambda\)</span>, we are forcing the parameter values to be smaller, which intuitively should result in a “simpler” model. In contrast, smaller <span class="math notranslate nohighlight">\(\lambda\)</span> will result in a more complex model. In fact, in the limit as <span class="math notranslate nohighlight">\(\lambda \to 0\)</span>, we actually will get a familiar solution back:</p>
<div class="math notranslate nohighlight">
\[
\lim_{\lambda\to 0}\hat{\boldsymbol{\beta}}_{RR}(\lambda) = \hat{\boldsymbol{\beta}}_{MP} = \boldsymbol{X}^\dagger\boldsymbol{y}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X}^\dagger\)</span> is the Moore-Penrose pseudo-inverse (discussed in a <a class="reference internal" href="../basic_linear_regression/more_least_squares.html"><span class="doc std std-doc">previous section</span></a>). Let’s first try visualizing the Ridge solutions across a variety of values of <span class="math notranslate nohighlight">\(\lambda\)</span>. Here we will fix <span class="math notranslate nohighlight">\(p=50\)</span> (which is bigger than <span class="math notranslate nohighlight">\(n=30\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span>

<span class="n">lamb_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span> 

<span class="c1"># get a colormap to represent the lambda values</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">norm</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">colors</span><span class="o">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">vmin</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">vmax</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">cmap</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">ScalarMappable</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">mpl</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">jet</span><span class="p">)</span>
<span class="n">cmap</span><span class="o">.</span><span class="n">set_array</span><span class="p">([])</span>

<span class="n">p</span><span class="o">=</span><span class="mi">50</span>
<span class="n">mses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">ix</span><span class="p">,</span> <span class="n">lamb</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">):</span>
    <span class="n">Xp</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span> <span class="c1"># generate feature matrix</span>
    <span class="n">beta_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@Xp</span> <span class="o">+</span> <span class="n">lamb</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="nd">@Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1"># get RR coefficients</span>

    <span class="c1"># make predictions on test samples to compare the fitted function</span>
    <span class="n">Xp_test</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">y_hat_test</span> <span class="o">=</span> <span class="n">Xp_test</span><span class="nd">@beta_hat</span>
    <span class="n">mses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_hat_test</span> <span class="o">-</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_hat_test</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">cmap</span><span class="o">.</span><span class="n">to_rgba</span><span class="p">(</span><span class="n">ix</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>

<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">cmap</span><span class="p">,</span> <span class="n">ticks</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;$\lambda$&quot;</span><span class="p">)</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([</span><span class="nb">round</span><span class="p">(</span><span class="n">l</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">lamb_range</span><span class="p">[::</span><span class="mi">10</span><span class="p">]])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f_\star$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_9_0.png" src="../../_images/nonlinear_regression_9_0.png" />
</div>
</div>
<p>As we can see from this plot, the solutions indeed become “simpler” as <span class="math notranslate nohighlight">\(\lambda\)</span> gets bigger. We can also plot the errors (MSEs) as a function of <span class="math notranslate nohighlight">\(\lambda\)</span> to see which value is theoretically best.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lamb_best</span> <span class="o">=</span> <span class="n">lamb_range</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">mses</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;best value of lambda is </span><span class="si">{</span><span class="n">lamb_best</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">,</span> <span class="n">mses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\lambda$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>best value of lambda is 0.141
</pre></div>
</div>
<img alt="../../_images/nonlinear_regression_11_1.png" src="../../_images/nonlinear_regression_11_1.png" />
</div>
</div>
<p>In turns out that the best MSE in this case occurs at <span class="math notranslate nohighlight">\(\lambda \approx 0.14\)</span>. We also see that for <span class="math notranslate nohighlight">\(\lambda\)</span> very small, the MSEs get significantly larger; in the limit, this corresponds exactly to the MSE of the MP solution. So, let’s plot the MP solution versus the <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\beta}}_{RR}(\lambda = 0.14)\)</span> solution to see how they compare.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Xp</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span> <span class="c1"># generate feature matrix</span>
<span class="n">beta_hat_RR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@Xp</span> <span class="o">+</span> <span class="n">lamb_best</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">Xp</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="nd">@Xp</span><span class="o">.</span><span class="n">T</span><span class="nd">@y</span> <span class="c1"># get RR coefficients</span>
<span class="n">beta_hat_MP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">Xp</span><span class="p">)</span><span class="nd">@y</span>
<span class="c1"># make predictions on test samples to compare the fitted function</span>
<span class="n">Xp_test</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">y_hat_test_RR</span> <span class="o">=</span> <span class="n">Xp_test</span><span class="nd">@beta_hat_RR</span>
<span class="n">y_hat_test_MP</span> <span class="o">=</span> <span class="n">Xp_test</span><span class="nd">@beta_hat_MP</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_hat_test_RR</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ridge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_hat_test_MP</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;MP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">f_star</span><span class="p">(</span><span class="n">x_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s2">&quot;$f_\star$&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_13_0.png" src="../../_images/nonlinear_regression_13_0.png" />
</div>
</div>
<p>As we can see, the MP solution performs completely different than the Ridge solution at <span class="math notranslate nohighlight">\(\lambda = 0.14\)</span>! It completely overfits the data, and does a very poor job of approximating the true regression function <span class="math notranslate nohighlight">\(f_\star\)</span>. We also observe another interesting point with the MP solution: it actually <em>perfectly</em> predicts on each of the data points it was fit on. Indeed, it will always do this: when <span class="math notranslate nohighlight">\(p&gt;n\)</span>, the Moore-Penrose pseudo-inverse is guaranteed to satisfy <span class="math notranslate nohighlight">\(\boldsymbol{X}\hat{\boldsymbol{\beta}}_{MP} = \boldsymbol{XX}^\dagger \boldsymbol{y} = \boldsymbol{y}\)</span> (mathematically, this is because it will be a right inverse for the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>). This phenomenon is referred to as “interpolation”, which is sometimes conflated with the concept of overfitting. This is not always the case; in the homework, we will investigate a very interesting (and only recently understood) phenomenon called “double descent”, in which performance actually <em>improves</em> when we interpolate the data.</p>
</div>
<div class="section" id="kernel-ridge-regression">
<h2><span class="section-number">3.2.2. </span>Kernel ridge regression<a class="headerlink" href="#kernel-ridge-regression" title="Permalink to this headline">¶</a></h2>
<p>So far, we’ve seen that the techniques from linear regression can be extended to non-linear function by fitting polynomials in one dimension. This general idea can be extended beyond polynomials and to multi-dimensional inputs. Consider for example when we have two features <span class="math notranslate nohighlight">\(\boldsymbol{x} = (x_1, x_2)\in \mathbb{R}^2\)</span>. Then one might define the feature map <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x})= (1, x_1,x_2,x_1x_2, x_1^2, x_2^2)\)</span> and fit the quadratic</p>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \beta_2x_2 + \beta_2x_1x_2 + \beta_4x_1^2 + \beta_5x_2^2 = \boldsymbol{\beta} \cdot \phi(\boldsymbol{x}).
\]</div>
<p>More generally, given features <span class="math notranslate nohighlight">\(\boldsymbol{x}\in \mathbb{R}^p\)</span> and a feature map <span class="math notranslate nohighlight">\(\phi:\mathbb{R}^p \to \mathbb{R}^N\)</span>, we can always fit the model <span class="math notranslate nohighlight">\(f\boldsymbol{x}) = \boldsymbol{\beta}^\top \phi(\boldsymbol{x})\)</span> using the usual techniques we have from linear regression.  Here we call <span class="math notranslate nohighlight">\(\mathbb{R}^N\)</span> the <em>feature space</em>.</p>
<p>To fit such a model to data <span class="math notranslate nohighlight">\(\boldsymbol{X}\in \mathbb{R}^{n\times p},\boldsymbol{y}\in \mathbb{R}^n\)</span>, we might hope to use our usual least-squares estimator</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}in \mathbb{R}^N}\|\boldsymbol{y} - \phi(\boldsymbol{X})\boldsymbol{\beta}\|_2^2
\]</div>
<p>where here <span class="math notranslate nohighlight">\(\phi(\boldsymbol{X})\in \mathbb{R}^{n\times N}\)</span> is the matrix whose <span class="math notranslate nohighlight">\(i\)</span>th row is <span class="math notranslate nohighlight">\(\phi(\boldsymbol{x}_i)\in \mathbb{R}^N\)</span>. Unfortunately, similar to what we saw with the polynomials, when <span class="math notranslate nohighlight">\(N &gt; n\)</span>, the matrix <span class="math notranslate nohighlight">\(\phi(\boldsymbol{X})\)</span> won’t be full rank, and so we won’t be able to  invert the matrix <span class="math notranslate nohighlight">\(\phi(\boldsymbol{X})^\top \phi(\boldsymbol{X})\)</span> to find the least squares solution. In this case, a natural solution is to consider the regularized problem</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}(\lambda) = \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^N} \|\boldsymbol{y} - \phi(\boldsymbol{X})\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2.
\]</div>
<p>As we’ve seen, this will yield the solution</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}(\lambda) = (\phi(\boldsymbol{X})^\top \phi(\boldsymbol{X}) + \lambda \boldsymbol{I})^{-1}\phi(\boldsymbol{X})^\top \boldsymbol{y}
\]</div>
<p>which always exists.</p>
<p>However, we now have another problem on our hands: we have to invert the matrix <span class="math notranslate nohighlight">\(\phi(\boldsymbol{X})^\top \phi(\boldsymbol{X}) + \lambda \boldsymbol{I}\)</span>, which can be very large when <span class="math notranslate nohighlight">\(N\)</span> is big. Fortunately, there is a way to get around this problem, using something known as the <em>kernel trick</em>.</p>
<p>Given a feature map <span class="math notranslate nohighlight">\(\phi: \mathbb{R}^p \to \mathbb{R}^N\)</span>, define the associated kernel function <span class="math notranslate nohighlight">\(k: \mathbb{R}^p\times \mathbb{R}^p \to \mathbb{R}\)</span> by</p>
<div class="math notranslate nohighlight">
\[
k(\boldsymbol{x},\boldsymbol{x}') = \langle \phi(\boldsymbol{x}),\phi(\boldsymbol{x}')\rangle.
\]</div>
<p><strong>Example.</strong> Consider the feature map <span class="math notranslate nohighlight">\(\phi(x_1,x_2) = (x_1^2, x_2^2, \sqrt{2}x_1x_2)\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
k(\boldsymbol{x},\boldsymbol{y}) = \langle \phi(\boldsymbol{x}), \phi(\boldsymbol{y})\rangle = x_1^2y_1^2 + x_2^2y_2^2 + 2x_1x_2y_1y_2 = (\boldsymbol{x} \cdot\boldsymbol{y})^2.
\]</div>
<p>The “kernel trick” is captured by the following fundamental result in kernel method, called the <em>representer theorem</em>.</p>
<p><strong>Theorem (representer theorem)</strong>. The function <span class="math notranslate nohighlight">\(\hat{f}(\boldsymbol{x}) = \hat{\boldsymbol{\beta}}^\top \phi(\boldsymbol{x})\)</span> resulting from the optimization problem</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\beta}}(\lambda) = \arg\min_{\boldsymbol{\beta}\in \mathbb{R}^N} \|\boldsymbol{y} - \phi(\boldsymbol{X})\boldsymbol{\beta}\|_2^2 + \lambda \|\boldsymbol{\beta}\|_2^2
\]</div>
<p>is always of the form <span class="math notranslate nohighlight">\(\hat{f}(\boldsymbol{x}) = \sum_{i=1}^n \hat{\alpha}_i k(\boldsymbol{x},\boldsymbol{x}_i)\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{\alpha}}(\lambda) = (\boldsymbol{K} + \lambda\boldsymbol{I})^{-1}\boldsymbol{y}.
\]</div>
<p>This means we only need to work with the <span class="math notranslate nohighlight">\(n\times n\)</span> matrix of dot products <span class="math notranslate nohighlight">\(\boldsymbol{K}\)</span> (which is called the Gram matrix, or the kernel matrix), which is much more efficient than working with the <span class="math notranslate nohighlight">\(N\times N\)</span> matrix <span class="math notranslate nohighlight">\(\phi(\boldsymbol{X})^\top \phi(\boldsymbol{X})\)</span> when <span class="math notranslate nohighlight">\(N \gg n\)</span>. Now that we’ve found <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\alpha}}\)</span>, we can get our fitted model</p>
<div class="math notranslate nohighlight">
\[
\hat{f}(\boldsymbol{x}) = \sum_{i=1}^n \hat{\alpha}_i k(\boldsymbol{x},\boldsymbol{x}_i).
\]</div>
<p>Note that this means we can fit models with features maps <span class="math notranslate nohighlight">\(\phi\)</span> <em>without ever having to work with feature map itself</em>. Instead, we just need access to the kernel function <span class="math notranslate nohighlight">\(k\)</span>. This is known as the kernel trick, because in many situations the kernel function <span class="math notranslate nohighlight">\(k\)</span> may be simple even when the feature map <span class="math notranslate nohighlight">\(\phi\)</span> is very high dimensional, and intractable to work with in practice. The general method of working with a kernel <span class="math notranslate nohighlight">\(k\)</span> rather than explicitly with a feature map <span class="math notranslate nohighlight">\(\phi\)</span> is called the “kernel trick”, and the specific method for regression presented here is called <em>kernel ridge regression</em>. Kernel ridge regression allows us to fit a very broad set of different functions using essentially the exact same tools as linear ridge regression.</p>
<p>Let’s see a real-world example. The <code class="docutils literal notranslate"><span class="pre">EGFR_bioactivity</span></code> dataset contains bioactivity data for <span class="math notranslate nohighlight">\(n = 2000\)</span> different drugs, each represented by a molecular “fingerprints”, which here is a binary vector of length <span class="math notranslate nohighlight">\(p = 512\)</span> indicating in each dimension whether the molecule contains a particular substructure or not. The response <code class="docutils literal notranslate"><span class="pre">y</span></code> is a measurement of how active each drug is against a particular protein (called EGFR) that it is targeting. The goal with this dataset is to build a model to predict this bioactivity from the features of the molecules. We load the dataset below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span> 

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;datasets/EGFR_bioactivity.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;X&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;y&quot;</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((2000, 512), (2000,))
</pre></div>
</div>
</div>
</div>
<p>To fit a kernel ridge regression model to this data, we first need to choose a kernel function <span class="math notranslate nohighlight">\(k\)</span>. For molecular fingerprints, a choice that is commonly made is the Jaccard kernel, which for two binary vectors <span class="math notranslate nohighlight">\(\boldsymbol{x},\boldsymbol{x}'\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
k_{Jaccard}(\boldsymbol{x},\boldsymbol{x}') = \frac{\boldsymbol{x}\cdot \boldsymbol{x}'}{\|\boldsymbol{x}\|^2_2 +\|\boldsymbol{x}'\|_2^2 - \boldsymbol{x}\cdot\boldsymbol{x}' }
\]</div>
<p>For comparison, we will also use the linear kernel</p>
<div class="math notranslate nohighlight">
\[
k_{linear}(\boldsymbol{x}, \boldsymbol{x}') = \boldsymbol{x}\cdot \boldsymbol{x}',
\]</div>
<p>which corresponds to doing usual linear ridge regression.</p>
<p>Let’s define a couple python functions that can compute these kernel for us. (These functions are designed to compute the entire kernel matrix, not just the value for single pair <span class="math notranslate nohighlight">\(\boldsymbol{x},\boldsymbol{x}'\)</span>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">k_jaccard</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">prod</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">norm_X</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">norm_Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prod</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_X</span> <span class="o">+</span> <span class="n">norm_Y</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">prod</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">k_linear</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">Y</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To fit the parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, we will need to split the data into two subsets: 1800 points are used for fitting the models, and 200 are used as a validation set to estimate the MSE for various values of the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>. We do this split below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1800</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_ix</span> <span class="o">=</span> <span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">j</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">train_ix</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_ix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_ix</span><span class="p">]</span>
<span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">test_ix</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_ix</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s also define a function to fit the kernel ridge regression model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_krr</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lamb</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">K</span><span class="o">+</span><span class="n">lamb</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="nd">@y</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can fit the models across a range of <span class="math notranslate nohighlight">\(\lambda\)</span> values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lamb_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>

<span class="n">K_jaccard_train</span> <span class="o">=</span> <span class="n">k_jaccard</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">K_jaccard_test</span> <span class="o">=</span> <span class="n">k_jaccard</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
<span class="n">K_linear_train</span> <span class="o">=</span> <span class="n">k_linear</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">K_linear_test</span> <span class="o">=</span> <span class="n">k_linear</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
<span class="n">mses_jaccard</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mses_linear</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">lamb</span> <span class="ow">in</span> <span class="n">lamb_range</span><span class="p">:</span>
    <span class="n">alpha_hat_jaccard</span> <span class="o">=</span> <span class="n">fit_krr</span><span class="p">(</span><span class="n">K_jaccard_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">lamb</span><span class="p">)</span>
    <span class="n">alpha_hat_linear</span> <span class="o">=</span> <span class="n">fit_krr</span><span class="p">(</span><span class="n">K_linear_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">lamb</span><span class="p">)</span>
    <span class="n">y_hat_jaccard</span> <span class="o">=</span> <span class="n">K_jaccard_test</span><span class="nd">@alpha_hat_jaccard</span> 
    <span class="n">y_hat_linear</span> <span class="o">=</span> <span class="n">K_linear_test</span><span class="nd">@alpha_hat_linear</span>
    <span class="n">mses_jaccard</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_test</span><span class="o">-</span><span class="n">y_hat_jaccard</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">mses_linear</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_test</span><span class="o">-</span><span class="n">y_hat_linear</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">,</span> <span class="n">mses_jaccard</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Jaccard kernel&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lamb_range</span><span class="p">,</span> <span class="n">mses_linear</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Linear kernel&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;MSE&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$\lambda$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/nonlinear_regression_23_0.png" src="../../_images/nonlinear_regression_23_0.png" />
</div>
</div>
<p>Interestingly, the optimal value of <span class="math notranslate nohighlight">\(\lambda\)</span> for the linear kernel (i.e. normal ridge regression) is almost two orders of magnitude higher than that for the model using the Jaccard kernel. Moreover, the Jaccard model attains lower MSE on the validation set. This is but one example of when using a kernel that’s not linear can improve performance in function estimation.</p>
<p>Before concluding this section, it is important to note a few cautions in fitting nonlinear models. First, a nonlinear model is not <em>always</em> better. In particular, as we’ve seen in many examples throughout this workbook, there are many real world datasets for which a linear model <em>is</em> appropriate. Second, a downside to working with a nonlinear model is that we lose a great deal of interpretability. In linear regression, the coefficients are easily interpretable: <span class="math notranslate nohighlight">\(\hat{\beta}_j\)</span> corresponds to the margin effect on the response of a unit change in feature <span class="math notranslate nohighlight">\(j\)</span>. This makes performing e.g. hypothesis tests natural. With complicated nonlinearities, we can no longer make such interpretations, which make performing statistical influence less meaningful. Nevertheless, when the priority is function approximation, nonlinear regression methods like kernel ridge regression can be effective tools.</p>
</div>
<div class="section" id="appendix-choosing-kernels">
<h2><span class="section-number">3.2.3. </span>Appendix: choosing kernels<a class="headerlink" href="#appendix-choosing-kernels" title="Permalink to this headline">¶</a></h2>
<p>An important choice that we need to make when using kernel ridge regression is to pick the kernel function <span class="math notranslate nohighlight">\(k\)</span>. Here we will see a few examples of kernels. Typically, we interpret kernels as measures of similarity.</p>
<p>Perhaps the most commonly used kernel is the <strong>Gaussian RBF kernel</strong>:</p>
<div class="math notranslate nohighlight">
\[
k(\boldsymbol{x},\boldsymbol{x}') = \exp(-\gamma \|\boldsymbol{x}-\boldsymbol{x}'\|_2^2)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\gamma&gt;0\)</span> is a <em>bandwidth</em> parameter that is chosen by the user. Larger <span class="math notranslate nohighlight">\(\gamma\)</span> corresponds to “localizing” more around individual points, since the similarity decays quickly as <span class="math notranslate nohighlight">\(\|\boldsymbol{x}-\boldsymbol{x}'\|_2^2\)</span> grows. On the other hand, smaller <span class="math notranslate nohighlight">\(\gamma\)</span> makes the similarity decay more slowly as <span class="math notranslate nohighlight">\(\|\boldsymbol{x}-\boldsymbol{x}'\|_2^2\)</span> grows. In practice, we tend to choose the parameter <span class="math notranslate nohighlight">\(\gamma\)</span> using cross validation.</p>
<p>Another kernel that one might use is the <strong>Laplace kernel</strong>:</p>
<div class="math notranslate nohighlight">
\[
k(\boldsymbol{x},\boldsymbol{x}') = \exp(-\gamma \|\boldsymbol{x}-\boldsymbol{x}'\|_1)
\]</div>
<p>We could also use <strong>polynomial kernels</strong>:</p>
<div class="math notranslate nohighlight">
\[
k(\boldsymbol{x},\boldsymbol{x}') = (\gamma \langle \boldsymbol{x}, \boldsymbol{x}'\rangle + 1)^p
\]</div>
<p>Here the degree <span class="math notranslate nohighlight">\(p\)</span> also needs to be chosen by the user.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content/generalizing_linear_regression"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="ridge_and_lasso.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3.1. </span>Regularizing regression: LASSO and Ridge</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../extensions_applications/chheader.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Extensions and applications</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>