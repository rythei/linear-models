
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.1. Principal Component Analysis (PCA) &#8212; Linear Algebra for Data Workbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.2. Sprectral Clustering" href="applications_spectral-clustering.html" />
    <link rel="prev" title="8. Applications" href="applications_chheader.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Linear Algebra for Data Workbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Welcome to Stat 89A
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="python_chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_1_chheader.html">
   2. Matrices, vectors, and
   <span class="math notranslate nohighlight">
    \(\mathbb{R}^n\)
   </span>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_introduction-to-norms.html">
     2.1. Introduction to Norms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_norms-integration-monte-carlo.html">
     2.2. An application: approximating integrals with norms and Monte Carlo integration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_lp-balls.html">
     2.3.
     <span class="math notranslate nohighlight">
      \(\ell_p\)
     </span>
     Balls
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_classification-with-norms.html">
     2.4. An application: classifying data points using norms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_2_chheader.html">
   3. Basics of vectors and vector spaces
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_2A_vectorspaces.html">
     3.1. Vectors and vector spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3_chheader.html">
   4. Basics of matrices
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices-and-matrix-operations.html">
     4.1. Matrices and matrix operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_deconstructing.html">
     4.2. Deconstructing Matrix Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_powers_of_matrices.html">
     4.3. Taking Powers of Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3B_chheader.html">
   5. Matrices as transformations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_linear_examples_new.html">
     5.1. Linear and Nonlinear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices_and_linear_functions.html">
     5.2. Matrices and Linear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_injective-and-surjective-functions.html">
     5.3. Injective, surjective and invertible functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_inverses.html">
     5.4. Left Inverses, Right Inverses, and Inverses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_changing_basis.html">
     5.5. Changing Basis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_4_chheader.html">
   6. Geometry: angles, orthogonality, and projections
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_dot-products-and-angles.html">
     6.1. Dot products, angles, and orthogonality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR.html">
     6.2. Gram–Schmidt and the QR Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR_linear_systems.html">
     6.3. Solving linear systems with the QR decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_projections.html">
     6.4. Projections
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The EVD, SVD and applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="eigenstuff_chheader.html">
   7. Eigenthings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_quadratic-forms.html">
     7.1. Quadratic forms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings.html">
     7.2. Eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings-special-matrices.html">
     7.3. The Eigenvalue decomposition for special types of matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_qr-algorithm.html">
     7.4. The QR algorithm for finding eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_SVD.html">
     7.5. The Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_low-rank-approximation.html">
     7.6. Low-rank approximation using the SVD
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="applications_chheader.html">
   8. Applications
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.1. Principal Component Analysis (PCA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_spectral-clustering.html">
     8.2. Sprectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_least-squares.html">
     8.3. Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_double-descent.html">
     8.4. The “double descent” phenomenon
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/content/applications_PCA.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/applications_PCA.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/applications_PCA.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   8.1.1. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics-of-pca">
   8.1.2. Basics of PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-role-of-the-eigenvalues">
   8.1.3. The role of the eigenvalues
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-from-the-svd">
   8.1.4. PCA from the SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-example-with-handwritten-digits">
   8.1.5. An example with handwritten digits
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Principal Component Analysis (PCA)</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation">
   8.1.1. Motivation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basics-of-pca">
   8.1.2. Basics of PCA
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-role-of-the-eigenvalues">
   8.1.3. The role of the eigenvalues
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca-from-the-svd">
   8.1.4. PCA from the SVD
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#an-example-with-handwritten-digits">
   8.1.5. An example with handwritten digits
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="principal-component-analysis-pca">
<h1><span class="section-number">8.1. </span>Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation">
<h2><span class="section-number">8.1.1. </span>Motivation<a class="headerlink" href="#motivation" title="Permalink to this headline">¶</a></h2>
<p>Dimensionality reduction is a fundamental concept in machine learning, aiming to exploit relationships among points in high-dimensional space in order to construct some low-dimensional summaries. By doing so, this process eliminates redundancies, while preserving interesting characteristics of the data. In turn, we can use dimensional reduction techniques to extract interesting features and improve the computational tractability of downstream machine learning algorithms as well as to visualize data which are comprised of many interrelated variables.</p>
<p>The idea of dimension reduction is really not new and dates back to Pearson(1901) and Hotelling (1933) who have invented principal component analysis (PCA). Today, PCA is the most widely used (linear) dimension reduction technique, due to its simple geometric interpretation. And all you need to know to understand PCA are some basic concepts from linear algebra.</p>
<p>To gain some intution, let’s consider an example in 2 dimensions. Here we consider two variables, <span class="math notranslate nohighlight">\(X_1\)</span> which is the weight of each of <span class="math notranslate nohighlight">\(n=145\)</span> students in the fifth grade, and <span class="math notranslate nohighlight">\(X_2\)</span> which is each student’s corresponding height. We’re interested in seeing how “related” the two variables are, e.g. in the sense that changes in the value of one variable “predict” changes in the value of the other variable.</p>
<p>First, let’s plot the sample points to get an idea of what we are working with.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">145</span><span class="p">)</span><span class="o">*</span><span class="mi">15</span> <span class="o">+</span> <span class="mi">150</span> <span class="c1"># weight (lbs)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">145</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span> <span class="o">+</span> <span class="mi">4</span> <span class="c1"># height (ft)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_PCA_1_0.png" src="../_images/applications_PCA_1_0.png" />
</div>
</div>
<p>To quantify whether there is some relationship between height and weight we could for example compute the (cosine of the) angle <span class="math notranslate nohighlight">\(\gamma\)</span> between two vectors to provide a notion of closeness.</p>
<div class="math notranslate nohighlight">
\[
 \gamma = \cos(\theta) = \frac{\boldsymbol{x}^\top \boldsymbol{y}}{\|\boldsymbol{x}\|_2\|\boldsymbol{y}\|_2} .
\]</div>
<p>If we mean center both <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> in addition, we obtain Pearson’s correlation coefficient <span class="math notranslate nohighlight">\(r_{XY}\)</span></p>
<div class="math notranslate nohighlight">
\[
 r_{XY} =  \frac{ (\boldsymbol{x}-\bar{x})^\top (\boldsymbol{y}-\bar{y})  }{\|\boldsymbol{x}-\bar{x}\|_2\|\boldsymbol{y}-\bar{y}\|_2},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}\)</span> is the sample mean</p>
<div class="math notranslate nohighlight">
\[ \bar{x} = \frac{1}{n} \sum_i x_i. \]</div>
<p>(If you don’t recall Pearson’s correlation coefficient, check out this wiki: <a class="reference external" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient.)">https://en.wikipedia.org/wiki/Pearson_correlation_coefficient.)</a></p>
<p>Let’s compute <span class="math notranslate nohighlight">\(r_{XY}\)</span> for our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># mean center x</span>
<span class="n">y</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># mean center y</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span> <span class="c1"># compute dot product between x and y</span>
<span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># euclidean norm of x</span>
<span class="n">y_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># euclidean norm of y</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">xy</span> <span class="o">/</span> <span class="p">(</span><span class="n">x_norm</span> <span class="o">*</span> <span class="n">y_norm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8781109535909715
</pre></div>
</div>
</div>
</div>
<p>So the correlation coefficient indicates that there is a relationship between the height and weight, i.e., if you only know the weight of a person, you can make an educated guess about the person’s height and vice versa. The closer the correlation coefficient is to 1, the more related the two variables.</p>
<p>Given that the variables height and weight seem to be very highly correlated, it is reasonable to ask whether we can form a new variable that summarizes both weight and height. A natural form for such a summary variable would be some linear combination of weight and height, e.g.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z = a X + b Y = \begin{bmatrix} a &amp; b \end{bmatrix}  \begin{bmatrix} X\\ Y\end{bmatrix}.
\end{split}\]</div>
<p>As we will see in the remainder of this section, principal component analysis allows us to construct such summary variables in a principled manner.</p>
</div>
<div class="section" id="basics-of-pca">
<h2><span class="section-number">8.1.2. </span>Basics of PCA<a class="headerlink" href="#basics-of-pca" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have observed <span class="math notranslate nohighlight">\(n\)</span> observations <span class="math notranslate nohighlight">\(\boldsymbol{x}_1,\dots,\boldsymbol{x}_n \in \mathbb{R}^p\)</span>, where each <span class="math notranslate nohighlight">\(x_{ij}\)</span> corresponds to the <span class="math notranslate nohighlight">\(j^{th}\)</span> feature for observation <span class="math notranslate nohighlight">\(i\)</span>. For convenience, we often represent this data as an <span class="math notranslate nohighlight">\(n\times p\)</span> “design matrix” given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \begin{bmatrix} - &amp; \boldsymbol{x}_1 &amp; - \\  &amp;\vdots &amp; \\ - &amp; \boldsymbol{x}_n &amp; - \end{bmatrix} \in \mathbb{R}^{n\times p}
\end{split}\]</div>
<p>In many circumstances, these <span class="math notranslate nohighlight">\(p\)</span> features may be highly correlated with one another. When <span class="math notranslate nohighlight">\(p &gt; 2\)</span> dimensions, there are two common ways to capture this correlation structure. The first is with the covariance matrix, which given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C} = \frac{1}{n}(\boldsymbol{X} - \bar{\boldsymbol{x}})^\top (\boldsymbol{X}-\bar{\boldsymbol{x}})
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{\boldsymbol{x}}\)</span> is the vector containing the means of each of the columns of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, i.e. <span class="math notranslate nohighlight">\(\bar{x}_j = \frac{1}{n}\sum_{i=1}^n x_{ij}\)</span>. Note that we are abusing notation by writing <span class="math notranslate nohighlight">\(\boldsymbol{X}-\bar{\boldsymbol{x}}\)</span>, as the first object is a matrix and the second is a vector, though what we really mean here is that we are substracting the number <span class="math notranslate nohighlight">\(\bar{x}_j\)</span> from each of the entries in the <span class="math notranslate nohighlight">\(j^{th}\)</span> column of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. Another way to compute this in practice is by defining the <em>centering matrix</em> <span class="math notranslate nohighlight">\(\boldsymbol{H} = \boldsymbol{I} - \frac{1}{n}\boldsymbol{11}^\top\)</span>, then we can mean-center the columns of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> by applying <span class="math notranslate nohighlight">\(\boldsymbol{H}\)</span> on the left, i.e. with <span class="math notranslate nohighlight">\(\boldsymbol{HX}\)</span>.</p>
<p>The covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> contains all the pairwise covariances between the features, so that</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C}_{jk} = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)(x_{ik} - \bar{x}_k).
\]</div>
<p>As we will see later, it will often make sense to consider a normalized version of the covariance matrix called the correlation matrix, which is given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{R} = \boldsymbol{D}^{-1/2}\boldsymbol{C}\boldsymbol{D}^{-1/2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{D}\)</span> is a diagonal matrix with <span class="math notranslate nohighlight">\(\boldsymbol{D}_{jj} = \frac{1}{n}\sum_{i=1}^n (x_{ij} - \bar{x}_j)^2\)</span>. Note that the correlation matrix corresponds to the covariance matrix associated with the normalized features <span class="math notranslate nohighlight">\(\tilde{X}_j = X_j/\sqrt{\text{Var}(X_j)}\)</span>. Note that both <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> are always symmetric matrices.</p>
<p>The goal of principal component analysis is to find <span class="math notranslate nohighlight">\(k \leq p\)</span> new summary variables that capture as much information in the original data as possible, but in fewer dimensions. Formally, we want to find new variables <span class="math notranslate nohighlight">\(\boldsymbol{z}_1,\dots,\boldsymbol{z}_k\)</span> which are linear combinations of the original features, of the form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{z}_1 &amp;=\boldsymbol{X}\boldsymbol{u}_1\\
\vdots \\
\boldsymbol{z}_k &amp;= \boldsymbol{X}\boldsymbol{u}_k.
\end{align*}
\end{split}\]</div>
<p>Here we require the following out of the new features:</p>
<ol class="simple">
<li><p>The summarize as much of the variation in the original data as possible</p></li>
<li><p>They are independent, in the sense that <span class="math notranslate nohighlight">\(\boldsymbol{u}_j^\top \boldsymbol{u}_l = 0\)</span> if <span class="math notranslate nohighlight">\(j\neq l\)</span></p></li>
<li><p>The vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_j\)</span> are unit vectors.</p></li>
</ol>
<p>To see how this works out, we can write down the optimization problem for the first feature <span class="math notranslate nohighlight">\(\boldsymbol{z}_1\)</span>.</p>
<div class="math notranslate nohighlight">
\[
\max_{\|\boldsymbol{u}_1\|_2 = 1} \text{Var}(\boldsymbol{Xu}_1)
\]</div>
<p>Now using some basic properties of variances, it is easy to show that</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(\boldsymbol{Xu}_1) = \boldsymbol{u}_1^\top \boldsymbol{C}\boldsymbol{u}_1
\]</div>
<p>which we recognize as a quadratic form associated with the symmetric matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span>. Plugging this into our original variance maximization problem, we find</p>
<div class="math notranslate nohighlight">
\[
\max_{\|\boldsymbol{u}_1\|_2 = 1} \text{Var}(\boldsymbol{Xu}_1) = \max_{\|\boldsymbol{u}_1\|_2 = 1} \boldsymbol{u}_1^\top \boldsymbol{C}\boldsymbol{u}_1
\]</div>
<p>This is an optimization problem we’ve seen before! The solution to this problem is exactly given by the top eigenvector (i.e. associated with the largest eigenvalue) of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span>. Moreover, the rest of the solutions of the rest of the vectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_2,\dots, \boldsymbol{u}_k\)</span> are just the next <span class="math notranslate nohighlight">\(k-1\)</span> eigenvectors. In the context of PCA, the eigenvectors <span class="math notranslate nohighlight">\(\boldsymbol{u}_j\)</span> are typically called the <em>loadings</em>, and the new features <span class="math notranslate nohighlight">\(\boldsymbol{z}_j = \boldsymbol{Xu}_j\)</span> are called the <em>scores</em>.</p>
<p>Hence the problem of PCA, defined is exactly solved by finding the eigenvalue decomposition of the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span>.</p>
<p>Let’s briefly return to the toy example of height and weight introduced earlier. First, let’s construct the design matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(145, 2)
</pre></div>
</div>
</div>
</div>
<p>Next, let’s construct the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>(recall that we already mean-centered the data). Next, lets compute the eigenvalue decomposition of the covariance matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigenvals</span><span class="p">,</span> <span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s visualize the top eigenvectors overlayed on top of our data. (Note we rescale them by a factor of 10 so that we can visualize it on the plot)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">origin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="c1"># plot the vector u1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="o">*</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">)</span>
<span class="c1"># plot the vector u2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="o">*</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_PCA_11_0.png" src="../_images/applications_PCA_11_0.png" />
</div>
</div>
<p>Strangely, this doesn’t seem to do as well as we might have hoped – the top eigenvector seems to be primarily aligned with the weight axis. It turns out this is primarily due to an issue of scale, in particular the fact that the scale on the weight feature (in lbs) is much larger than the scale of the height feature (in ft). This can be resolved by using the correlation matrix instead of the covariance matrix. So, let’s repeat the above steps but using the normalized features instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>
<span class="n">y</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mf">0.5</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">eigenvals</span><span class="p">,</span> <span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>

<span class="n">origin</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="c1"># plot the vector u1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="o">*</span><span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">)</span>
<span class="c1"># plot the vector u2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">quiver</span><span class="p">(</span><span class="o">*</span><span class="n">origin</span><span class="p">,</span> <span class="o">*</span><span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;g&#39;</span><span class="p">],</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_PCA_13_0.png" src="../_images/applications_PCA_13_0.png" />
</div>
</div>
<p>Now the first eigenvector seems to be aligned with the direction corresponding to the most variation in the data, which was exactly the goal of the PCA. In general, the issue of having features with different scales appears frequently in practice, and so it is always recommend to work with normalized features, and hence the correlation matrix, rather than the unnormalized covariance matrix.</p>
</div>
<div class="section" id="the-role-of-the-eigenvalues">
<h2><span class="section-number">8.1.3. </span>The role of the eigenvalues<a class="headerlink" href="#the-role-of-the-eigenvalues" title="Permalink to this headline">¶</a></h2>
<p>Previously we saw that the eigenvectors of the covariance/correlation represent the directions of maximal variation in the data. It turns out that the eigenvalues also have a valuable interpretation when performing PCA. First, we observe that the variance of the <span class="math notranslate nohighlight">\(j^{th}\)</span> feature <span class="math notranslate nohighlight">\(\boldsymbol{z}_j\)</span> is exactly equal to the <span class="math notranslate nohighlight">\(j^{th}\)</span> eigenvalue:</p>
<div class="math notranslate nohighlight">
\[
\text{Var}(\boldsymbol{z}_j) = \text{Var}(\boldsymbol{X}\boldsymbol{u}_j) = \boldsymbol{u^\top_j C u_j} = \lambda_j.
\]</div>
<p>Moreover, the total amount of variance in the data is given by</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^p \boldsymbol{C}_{jj} = \text{Trace}(\boldsymbol{C}) = \sum_{j=1}^p \lambda_j.
\]</div>
<p>Therefore, we can intepret the ratio</p>
<div class="math notranslate nohighlight">
\[
\frac{\lambda_m}{\sum_{j=1}^p \lambda_j}
\]</div>
<p>as the <em>proportion of variance described by the <span class="math notranslate nohighlight">\(m^{th}\)</span> component</em>. This is very valuable in practice when selecting how many dimensions we want to use. For example, in a simple height/weight problem, the proportion of variance described by the first direction is</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">eigenvals</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9918782443031121
</pre></div>
</div>
</div>
</div>
<p>This tells us that we can explain more than 99% of the variance in the data by just considering the first component! This suggests that this data can be very well described using only one dimension (i.e. the feature <span class="math notranslate nohighlight">\(\boldsymbol{z}_1\)</span>) rather than the original two features height and weight.</p>
<p>Before proceeding to show a more realistic application of PCA, we will see that all the above steps can equivalently be performed using the singular value decomposition.</p>
</div>
<div class="section" id="pca-from-the-svd">
<h2><span class="section-number">8.1.4. </span>PCA from the SVD<a class="headerlink" href="#pca-from-the-svd" title="Permalink to this headline">¶</a></h2>
<p>In this section, we briefly show how the principal component analysis as derived above can equivalently be stated in terms of the singular value decomposition.</p>
<p>Recall that the SVD writes an <span class="math notranslate nohighlight">\(n\times p\)</span> matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> as <span class="math notranslate nohighlight">\(\boldsymbol{X} = \boldsymbol{U\Sigma V}^\top\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{U}\)</span> are the eigenvectors obtained from the EVD of <span class="math notranslate nohighlight">\(\boldsymbol{XX^\top}\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{V}\)</span> are the eigenvectors obtained from the EVD of <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> contains the square-roots of the non-zero eigenvalues from either <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{XX^\top}\)</span>. Now remember from above that we found the principal components by considering the eigenvalue decomposition of the matrix</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{C} = \frac{1}{n}\boldsymbol{X}_c^\top\boldsymbol{X}_c
\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X}_c = \boldsymbol{HX}\)</span> is the data matrix with the columns mean-centered. Then the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> are the same as the right singular vectors of <span class="math notranslate nohighlight">\(\boldsymbol{X}_c\)</span>, and the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{C}\)</span> are given by <span class="math notranslate nohighlight">\(\lambda_i = \frac{1}{n}\sigma_i^2\)</span>, where <span class="math notranslate nohighlight">\(\sigma_i\)</span> is the <span class="math notranslate nohighlight">\(i^{th}\)</span> singular value of <span class="math notranslate nohighlight">\(\boldsymbol{X}_c\)</span>.</p>
</div>
<div class="section" id="an-example-with-handwritten-digits">
<h2><span class="section-number">8.1.5. </span>An example with handwritten digits<a class="headerlink" href="#an-example-with-handwritten-digits" title="Permalink to this headline">¶</a></h2>
<p>Here we will apply PCA to a dataset containing images of handwritten digits. We can load and visualize the dataset using the following:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">target</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;X has shape </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">for</span> <span class="n">ix</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;label = </span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X has shape (1797, 64)
</pre></div>
</div>
<img alt="../_images/applications_PCA_17_1.png" src="../_images/applications_PCA_17_1.png" />
</div>
</div>
<p>Note that the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> has shape <span class="math notranslate nohighlight">\(1797 \times 64\)</span>, where <span class="math notranslate nohighlight">\(64\)</span> corresponds to the fact that each image is <span class="math notranslate nohighlight">\(8\times 8\)</span>. Next, let’s mean-center the data matrix and compute the covariance matrix.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># centering matrix</span>
<span class="n">ones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">ones</span><span class="p">,</span> <span class="n">ones</span><span class="p">)</span>

<span class="c1"># mean-center data by applying the centering matrix</span>
<span class="n">Xc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

<span class="c1"># compute the covariance matrix</span>
<span class="n">C</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xc</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Xc</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we can compute the eigenvalue decomposition of the covariance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eigenvals</span><span class="p">,</span> <span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eig</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>

<span class="c1"># make sure the eigenvalues are sorted largest to smallest</span>
<span class="n">sorted_ix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">))</span>
<span class="n">eigenvals</span> <span class="o">=</span> <span class="n">eigenvals</span><span class="p">[</span><span class="n">sorted_ix</span><span class="p">]</span>
<span class="n">eigenvecs</span> <span class="o">=</span> <span class="n">eigenvecs</span><span class="p">[</span><span class="n">sorted_ix</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>How many dimensions do we need to faithfully represent this dataset? One way to get an understanding of this is by plotting the proportion of variance described by each dimension. Recall that this is given by <span class="math notranslate nohighlight">\(\lambda_m / \sum_{j} \lambda_j\)</span>. Such a plot is called a <em>scree plot</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">eigenvals</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eigenvals</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;dimension&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;prop. of variance explained&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;scree plot&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_PCA_23_0.png" src="../_images/applications_PCA_23_0.png" />
</div>
</div>
<p>We can see that the first few dimensions do seem to capture much of the variance in the data. Let’s try computing and visualizing the first three principal components.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>

<span class="c1"># compute the first 3 PCs by projecting the data onto the top 3 eigenvectors</span>
<span class="n">z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">z3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">eigenvecs</span><span class="p">[:,</span><span class="mi">2</span><span class="p">])</span>

<span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">z1</span><span class="p">[</span><span class="n">labels</span><span class="o">==</span><span class="n">l</span><span class="p">],</span><span class="n">z2</span><span class="p">[</span><span class="n">labels</span><span class="o">==</span><span class="n">l</span><span class="p">],</span><span class="n">z3</span><span class="p">[</span><span class="n">labels</span><span class="o">==</span><span class="n">l</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Z1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Z2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s1">&#39;Z3&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_PCA_25_0.png" src="../_images/applications_PCA_25_0.png" />
</div>
</div>
<p>As we can see from the plot, the principal components provide a nice way to visualize data that is high dimensional.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="applications_chheader.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8. </span>Applications</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="applications_spectral-clustering.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.2. </span>Sprectral Clustering</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney, N. Benjamin Erichson and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>