
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8.3. Least Squares &#8212; Linear Algebra for Data Workbook</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8.4. The “double descent” phenomenon" href="applications_double-descent.html" />
    <link rel="prev" title="8.2. Sprectral Clustering" href="applications_spectral-clustering.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Linear Algebra for Data Workbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../overview.html">
   Welcome to Stat 89A
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="python_chheader.html">
   1. Python 101
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="python_basics.html">
     1.1. The Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_numpy.html">
     1.2. Introduction to NumPy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="python_plotting.html">
     1.3. MatPlotLib
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Linear Algebra
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_1_chheader.html">
   2. Matrices, vectors, and
   <span class="math notranslate nohighlight">
    \(\mathbb{R}^n\)
   </span>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_introduction-to-norms.html">
     2.1. Introduction to Norms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_norms-integration-monte-carlo.html">
     2.2. An application: approximating integrals with norms and Monte Carlo integration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_lp-balls.html">
     2.3.
     <span class="math notranslate nohighlight">
      \(\ell_p\)
     </span>
     Balls
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_1_classification-with-norms.html">
     2.4. An application: classifying data points using norms
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_2_chheader.html">
   3. Basics of vectors and vector spaces
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_2A_vectorspaces.html">
     3.1. Vectors and vector spaces
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3_chheader.html">
   4. Basics of matrices
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices-and-matrix-operations.html">
     4.1. Matrices and matrix operations
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_deconstructing.html">
     4.2. Deconstructing Matrix Multiplication
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_powers_of_matrices.html">
     4.3. Taking Powers of Matrices
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_3B_chheader.html">
   5. Matrices as transformations
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_linear_examples_new.html">
     5.1. Linear and Nonlinear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_matrices_and_linear_functions.html">
     5.2. Matrices and Linear Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_injective-and-surjective-functions.html">
     5.3. Injective, surjective and invertible functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_inverses.html">
     5.4. Left Inverses, Right Inverses, and Inverses
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_3_changing_basis.html">
     5.5. Changing Basis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="basicLA_4_chheader.html">
   6. Geometry: angles, orthogonality, and projections
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_dot-products-and-angles.html">
     6.1. Dot products, angles, and orthogonality
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR.html">
     6.2. Gram–Schmidt and the QR Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_QR_linear_systems.html">
     6.3. Solving linear systems with the QR decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="basicLA_4_projections.html">
     6.4. Projections
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The EVD, SVD and applications
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="eigenstuff_chheader.html">
   7. Eigenthings
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_quadratic-forms.html">
     7.1. Quadratic forms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings.html">
     7.2. Eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_eigenthings-special-matrices.html">
     7.3. The Eigenvalue decomposition for special types of matrices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_qr-algorithm.html">
     7.4. The QR algorithm for finding eigenvalues and eigenvectors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_SVD.html">
     7.5. The Singular Value Decomposition
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="eigenstuff_low-rank-approximation.html">
     7.6. Low-rank approximation using the SVD
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="applications_chheader.html">
   8. Applications
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="applications_PCA.html">
     8.1. Principal Component Analysis (PCA)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_spectral-clustering.html">
     8.2. Sprectral Clustering
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     8.3. Least Squares
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="applications_double-descent.html">
     8.4. The “double descent” phenomenon
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/content/applications_least-squares.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/applications_least-squares.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/erichson/LinearAlgebra/master?urlpath=tree/content/applications_least-squares.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation-simple-linear-regression">
   8.3.1. Motivation: simple linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deriving-the-least-squares-solution">
   8.3.2. Deriving the least squares solution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-curve-fitting-with-least-squares">
   8.3.3. Nonlinear curve fitting with least squares
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Least Squares</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#motivation-simple-linear-regression">
   8.3.1. Motivation: simple linear regression
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deriving-the-least-squares-solution">
   8.3.2. Deriving the least squares solution
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#nonlinear-curve-fitting-with-least-squares">
   8.3.3. Nonlinear curve fitting with least squares
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="least-squares">
<h1><span class="section-number">8.3. </span>Least Squares<a class="headerlink" href="#least-squares" title="Permalink to this headline">¶</a></h1>
<div class="section" id="motivation-simple-linear-regression">
<h2><span class="section-number">8.3.1. </span>Motivation: simple linear regression<a class="headerlink" href="#motivation-simple-linear-regression" title="Permalink to this headline">¶</a></h2>
<p>The motivation of most sciences is to learn new aspects about real-world phenomena. This is often done by studying how different variables are related to each other. We have already seen how we can use the the correlation coefficient to quantify the relationship between two variables. However, often we would like to understand how a random variable <span class="math notranslate nohighlight">\(Y\)</span> depends on some measurement variable (feature) <span class="math notranslate nohighlight">\(X\)</span>. More concretely, assume that we are interested to study the relationship between weight and height of a person. Here, it is reasonable to assume that weight depends on the height of person. Thus, we can formulate the following model</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}
\text{weight} = f(\text{height}).
\end{equation}
\]</div>
<p>It might be that a shorter person implies a lower weight. However, we cannot be certain about this assertion, since we do not know the function <span class="math notranslate nohighlight">\(f(\cdot)\)</span>. Regression analysis provides us with a powerful framework to address and study problems of this type. The most simple regression model assumes that the functional relationship between the response and explanatory variable is linear, i.e., <span class="math notranslate nohighlight">\(f(X) = a + Xb\)</span>. (Note, technically this is an afine function, but we simply call it a linear function.) The simple linear regression model can be formulated as</p>
<div class="math notranslate nohighlight">
\[
\begin{equation}\label{eq:YfXe}
Y = a + Xb + \varepsilon,
\end{equation}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\varepsilon\)</span> denotes the random error introduced by this model (so that the data doesn’t lie <em>perfectly</em> on a line).</p>
<p>The regression parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> can be estimated via the method of least squares. Here, we focus on the mechanics for computing the parameters and we are going to ignore the discussion of all the assumptions that are typically made by a statistican.</p>
<p>To gain some intution, we revisit the toy problem that we have already seen before. We consider two variables: <span class="math notranslate nohighlight">\(X_1\)</span> is the weight of each of <span class="math notranslate nohighlight">\(n=145\)</span> students, and <span class="math notranslate nohighlight">\(X_2\)</span> is each student’s corresponding height.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">145</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="mi">15</span> <span class="o">+</span> <span class="mi">150</span> <span class="c1"># weight (lbs)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">145</span><span class="p">)</span><span class="o">*</span><span class="mf">0.1</span> <span class="o">+</span> <span class="mi">4</span> <span class="c1"># height (ft)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_1_0.png" src="../_images/applications_least-squares_1_0.png" />
</div>
</div>
<p>Now, we want to learn a model that provides us with a prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> for a given input <span class="math notranslate nohighlight">\(x\)</span></p>
<div class="math notranslate nohighlight">
\[
\hat{y} = a + x b.
\]</div>
<p>This model describes a line and a naive approach would be to chose the parameters by hand. If we look at the data, we can make an educated guess by picturing that a line that fits the data has a slope of about <span class="math notranslate nohighlight">\(0.01\)</span> and if set <span class="math notranslate nohighlight">\(x=0\)</span> the line will intersect the y-axis at about 4.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">0.01</span>

<span class="n">ypred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">110</span><span class="p">,</span><span class="mi">190</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">110</span><span class="p">,</span><span class="mi">190</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_3_0.png" src="../_images/applications_least-squares_3_0.png" />
</div>
</div>
<p>This looks already like a reasonable model for the data, though we realize that the model is not plausible for data points outside of the observed range (of course, it makes no sense that someone who has zero weight is about 4 feet tall). To quote Box: “All models are wrong, but some are useful”. Formulating a “useful” model can be challenging and this is something that you will dicuss in a course on Statistical Modelling.</p>
<p>In the following we want to study a principled way to compute the parameters for a given model (independent of the question whether it is useful or not). A natural starting point to do so is to aim to minimize the distance between the predicted outcome <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the actual observed outcome <span class="math notranslate nohighlight">\(y\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\text{minimize} \,\, Q = \sum_i^n (y_i - \hat{y}_i)^2.
\]</div>
<p>Here we use the squared distance, since it doesn’t matter whether the distance is positive or negative.</p>
<p>Now, let’s plug in our model <span class="math notranslate nohighlight">\(\hat{y} = a + x b\)</span> so that we yield</p>
<div class="math notranslate nohighlight">
\[Q= \sum_i^b (y_i - (a + x_i b))^2.\]</div>
<p>Next, we can use calculus to find the parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. Specifically, <span class="math notranslate nohighlight">\(Q\)</span> will be minimized at the values of <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> for which <span class="math notranslate nohighlight">\(\frac{\partial Q}{\partial a}=0\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial Q}{\partial b}=0\)</span>. We yield the following two conditions:</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; \frac{\partial Q}{\partial a} = \sum_i^n -2 (y_i - a - x_i b) = 2na -2 \sum_i^n y_i + 2b \sum_i^n x_i = 0
\end{align}
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
&amp; \frac{\partial Q}{\partial b} = \sum_i^n -2 x_i (y_i - a - x_i b) = 2 a \sum_i^n x_i  -2 \sum_i^n x_i y_i + 2b \sum_i^n x_i^2 = 0.
\end{align}
\]</div>
<p>Now, we can first solve the first equation for for <span class="math notranslate nohighlight">\(a\)</span></p>
<div class="math notranslate nohighlight">
\[
\begin{align}
a = \bar{y} - b \bar{x}.
\end{align}
\]</div>
<p>Then, we can substitue <span class="math notranslate nohighlight">\(a\)</span> into the second equation and solve for <span class="math notranslate nohighlight">\(b\)</span></p>
<div class="math notranslate nohighlight">
\[
\begin{align}
b = \frac{\frac{1}{n} \sum_i^n (x_i - \bar{x})(y_i - \bar{y}) }{\frac{1}{n} \sum_i^n (x_i - \bar{x})^2} = \frac{Cov(x,y)}{Var(x)}
\end{align}
\]</div>
<p>Now, we can compute <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> for our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xbar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ybar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">xbar</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">ybar</span><span class="p">))</span> <span class="o">/</span>  <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">xbar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">ybar</span> <span class="o">-</span> <span class="n">b</span><span class="o">*</span><span class="n">xbar</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.010352211893002648
3.934880810191448
</pre></div>
</div>
</div>
</div>
<p>Well, it turns out that our original guess was not too bad. Let’s plot the regression line for these estimates.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ypred</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">110</span><span class="p">,</span><span class="mi">190</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">110</span><span class="p">,</span><span class="mi">190</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;weight (lbs)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;height (ft)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_7_0.png" src="../_images/applications_least-squares_7_0.png" />
</div>
</div>
</div>
<div class="section" id="deriving-the-least-squares-solution">
<h2><span class="section-number">8.3.2. </span>Deriving the least squares solution<a class="headerlink" href="#deriving-the-least-squares-solution" title="Permalink to this headline">¶</a></h2>
<p>The above method to compute the parameters <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> isn’t very elegant. Fortunately, we can use linear algebra to express the problem more concisely and to obtain the same results. More generally, let’s consider fitting a line with <span class="math notranslate nohighlight">\(p\)</span> different variables:</p>
<div class="math notranslate nohighlight">
\[
y = b_0 + b_1x_1 + \cdots + b_px_p + \varepsilon.
\]</div>
<p>where here <span class="math notranslate nohighlight">\(\varepsilon\)</span> is some noise, so that our data doesn’t lie <em>exactly</em> on a line.</p>
<p>If we define the vectors <span class="math notranslate nohighlight">\(\boldsymbol{b} = \begin{bmatrix}b_0 &amp; b_1 &amp; \dots &amp;b_p\end{bmatrix}^\top\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x} = \begin{bmatrix}1 &amp; x_1 &amp; \dots &amp; x_p\end{bmatrix}^\top\)</span> , this can be more concisely written as</p>
<div class="math notranslate nohighlight">
\[
y = \boldsymbol{b^\top x} + \varepsilon.
\]</div>
<p>(Note: we add the extra <span class="math notranslate nohighlight">\(1\)</span> to the <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> vector so that we can conveniently express the constant term in the linear equation in a dot product.)
Now suppose we have <span class="math notranslate nohighlight">\(n\)</span> datapoints <span class="math notranslate nohighlight">\((\boldsymbol{x}_1,y_1), \dots, (\boldsymbol{x}_n,y_n)\)</span> (where remember each <span class="math notranslate nohighlight">\(\boldsymbol{x}_i \in \mathbb{R}^{p+1}\)</span>) that we want to use to fit the parameters <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>. Let’s store our <span class="math notranslate nohighlight">\(n\)</span> data samples in an <span class="math notranslate nohighlight">\(n\times (p+1)\)</span> matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{X} = \begin{bmatrix} - &amp;\boldsymbol{x}_1 &amp; - \\ - &amp;\boldsymbol{x}_2 &amp; - \\ &amp;\vdots &amp; \\ - &amp;\boldsymbol{x}_n &amp; -\end{bmatrix} \in \mathbb{R}^{n\times (p+1)}
\end{split}\]</div>
<p>and all the <span class="math notranslate nohighlight">\(y_i\)</span> values in an <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n\end{bmatrix} \in \mathbb{R}^n.
\end{split}\]</div>
<p>Now our task can be written as finding <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}}\)</span> such that <span class="math notranslate nohighlight">\(\boldsymbol{X}\hat{\boldsymbol{b}} \approx \boldsymbol{y}\)</span>. We formulize this as the following optimization problem:</p>
<div class="math notranslate nohighlight">
\[
\min_\boldsymbol{b} \|\boldsymbol{Xb} - \boldsymbol{y}\|_2^2.
\]</div>
<p>This is called a <em>least squares</em> minimization problem. To solve this minimization problem, we can use a bit of matrix calculus (we didn’t cover this, but if you’re interested see e.g. <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_calculus">https://en.wikipedia.org/wiki/Matrix_calculus</a> – for the most part, there are basic formulas for the derivatives of most standard vector functions.)</p>
<p>Let’s start by expanding <span class="math notranslate nohighlight">\(\|\boldsymbol{Xb} - \boldsymbol{y}\|_2^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\|\boldsymbol{Xb} - \boldsymbol{y}\|_2^2 &amp;= (\boldsymbol{Xb} - \boldsymbol{y})^\top (\boldsymbol{Xb}-\boldsymbol{y}) \\&amp;= \boldsymbol{b^\top X^\top X b} - \boldsymbol{y^\top Xb} - \boldsymbol{b^\top X^\top y} + \boldsymbol{y^\top y} \\&amp;= \boldsymbol{b^\top X^\top X b} - 2\boldsymbol{y^\top Xb} + \boldsymbol{y^\top y}
\end{align*}
\end{split}\]</div>
<p>Now if we take the derivative with respect to <span class="math notranslate nohighlight">\(b\)</span> and set it equal to zero, we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\nabla_\boldsymbol{b} [\boldsymbol{b^\top X^\top X b} - 2\boldsymbol{b^\top X^\top y} + \boldsymbol{y^\top y}] &amp;= 2\boldsymbol{X^\top X b} - 2\boldsymbol{X^\top y} = 0\\ &amp;\implies \boldsymbol{X^\top X b} = \boldsymbol{X^\top y}
\end{align*}
\end{split}\]</div>
<p>The linear equations <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X b} = \boldsymbol{X^\top y}\)</span> defining the solutions to the least squares problem are called the <em>normal equations</em>. Depending on the situation, there are a variety of ways that we might go about solving the normal equations.</p>
<p><strong>Case 1: <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is full rank, so <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is invertible</strong></p>
<p>In the simplest case, the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is full rank, which means that it is invertible. In this case, we can straightforwardly solve the normal equations:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X^\top X b} = \boldsymbol{X^\top y} \implies \hat{\boldsymbol{b}} = (\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top y}.
\]</div>
<p>Then the predicted values are <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}} = \boldsymbol{X}\hat{\boldsymbol{b}} = \boldsymbol{X}(\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top y}\)</span>. The term <span class="math notranslate nohighlight">\(\boldsymbol{X}(\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top y}\)</span> should look familar: it is exactly the projection onto the column space that we have seen before. This makes good sense: if we write <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}} = \boldsymbol{X}\hat{\boldsymbol{b}}\)</span>, then we know that by definition <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}\)</span> is a vector in the column space of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> (the span of the columns of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>), which we’ll denote <span class="math notranslate nohighlight">\(\text{Col}(\boldsymbol{X})\)</span>. Hence we can rewrite the least squares problem as</p>
<div class="math notranslate nohighlight">
\[
\min_{\hat{\boldsymbol{y}}\in \text{Col}(\boldsymbol{X})} \|\hat{\boldsymbol{y}} - \boldsymbol{y}\|_2^2.
\]</div>
<p>The solution – finding the nearest point to <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> in the subspace <span class="math notranslate nohighlight">\(\text{Col}(\boldsymbol{X})\)</span> – is by definition the projection of of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> onto <span class="math notranslate nohighlight">\(\text{Col}(\boldsymbol{X])\)</span>.</p>
<p>In general, the matrix <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> will typically be full rank whenever <span class="math notranslate nohighlight">\(p &gt; n\)</span>. Let’s see a simple example of fitting the linear regression solution in this case. First, we’ll generate some fake data: our <span class="math notranslate nohighlight">\(\boldsymbol{x}_i\)</span>’s are from a normal distribution, and our <span class="math notranslate nohighlight">\(y_i\)</span>’s are generated as <span class="math notranslate nohighlight">\(y_i = \boldsymbol{b}_\star^\top \boldsymbol{x}_i + \varepsilon_i\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{b}_\star\)</span> is some fixed “ground truth” which we’re trying to recover.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#make the first column all ones</span>

<span class="n">b_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span> <span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="c1">#noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">b_star</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can find the least squares solutionn using <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = (\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top y}\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>A simple way to evaluate how good our estimate is is to calculate <span class="math notranslate nohighlight">\(\|\hat{\boldsymbol{b}}-\boldsymbol{b}_\star\|_2^2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">error</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b_hat</span><span class="o">-</span><span class="n">b_star</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The error is </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The error is 0.0014943894213478
</pre></div>
</div>
</div>
</div>
<p><strong>Case 2: <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is full rank, but <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> isn’t easily inverted</strong></p>
<p>One issue that can arise in practice is that while <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is <em>technically</em> full rank, it isn’t easy and/or stable to compute the inverse in practice for numerical reasons (for example, this is the case when <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> has some very small eigenvalues). One option in this situation is to use the QR decomposition, similar to the way we did in Section 6.3. Recall the QR decomposition writes <span class="math notranslate nohighlight">\(\boldsymbol{X} = \boldsymbol{QR}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{Q}\)</span> is an orthogonal matrix with <span class="math notranslate nohighlight">\(\boldsymbol{Q^\top Q} = \boldsymbol{I}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is an upper triangular matrix. Then if we plug <span class="math notranslate nohighlight">\(\boldsymbol{X} = \boldsymbol{QR}\)</span> into the normal equations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\boldsymbol{X^\top X b} = \boldsymbol{X^\top y} &amp;\iff (\boldsymbol{QR})^\top \boldsymbol{QR b} = (\boldsymbol{QR})^\top \boldsymbol{y} \\&amp;\iff \boldsymbol{R}^\top \underbrace{\boldsymbol{Q^\top Q}}_{=\boldsymbol{I}} \boldsymbol{R b} = \boldsymbol{R^\top Q^\top y} \\&amp;\iff \boldsymbol{Rb} = \boldsymbol{Q^\top y}
\end{align*}
\end{split}\]</div>
<p>Now we have reduced the problem to one of the form <span class="math notranslate nohighlight">\(\boldsymbol{Rb} = \boldsymbol{y}'\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{R}\)</span> is upper triangular, so we can use the back substitution algorithm to solve it. Let’s use this approach to solve a least squares problem. We’re going to build an example where <span class="math notranslate nohighlight">\(\boldsymbol{X^\top }\)</span> isn’t numerically very stable. To do this, we construct construct a <span class="math notranslate nohighlight">\(4\times 3\)</span> matrix whose third column is a small purturbation of the sum of the first two columns. Then we generate <span class="math notranslate nohighlight">\(\boldsymbol{y} = \boldsymbol{Xb}_\star + \boldsymbol{\varepsilon}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4</span>

<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">x3</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">+</span> <span class="mf">0.00001</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">,</span><span class="n">x3</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">b_star</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s start by computing a QR decomposition of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span><span class="p">,</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we use <code class="docutils literal notranslate"><span class="pre">scipy.linalg.solve_triangular</span></code> to perform back substution and solve for <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}}_{qr}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy</span>

<span class="n">b_hat_qr</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s check that this actually gave us a good solution to the normal equations by computing <span class="math notranslate nohighlight">\(\|\boldsymbol{X^\top X}\hat{\boldsymbol{b}}_{qr} - \boldsymbol{X^\top y}\|_2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inversion_error_qr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">b_hat_qr</span><span class="p">))</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">inversion_error_qr</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.728207101820981e-10
</pre></div>
</div>
</div>
</div>
<p>This is a pretty small error <span class="math notranslate nohighlight">\(\approx 4.4 \times 10^{-10}\)</span>. Now let’s check that the usual method via inverting <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> does not do as well on this problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b_hat_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">X</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
<span class="n">inversion_error_ls</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">b_hat_ls</span><span class="p">))</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span><span class="n">y</span><span class="p">))</span>
<span class="n">inversion_error_ls</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0019009385213144508
</pre></div>
</div>
</div>
</div>
<p>The error now is much larger – almost <span class="math notranslate nohighlight">\(8\)</span> orders of magnitude larger than the QR method.</p>
<p>A second approach that one might use in the situation when <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> isn’t invertible numerically is to use the so-called <em>ridge regression</em> solution. This solves a slightly different problem than the basic least squares problem, and gives us a solution of the form</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{b}} = (\boldsymbol{X^\top X} + \lambda \boldsymbol{I})^{-1}\boldsymbol{X^\top y}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is some parameter that we choose (and when <span class="math notranslate nohighlight">\(\lambda =0\)</span> we get the original least squares solution back). This is convenient since often in practice adding a small multiple of the identity matrix to <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> suffices to make <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X} + \lambda \boldsymbol{I}\)</span> more stable numerically (it essentially amounts to adding <span class="math notranslate nohighlight">\(\lambda\)</span> to all of the eigenvalues of <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span>). Ridge regression also has many nice statistical properties, but for the sake of this class we won’t discuss this aspect.</p>
<p><strong>Case 3: <span class="math notranslate nohighlight">\(X^\top X\)</span> is not full rank</strong></p>
<p>Another situation that can arise, particularly when <span class="math notranslate nohighlight">\(n&lt; p\)</span>, is that <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> is not full rank, and therefore not invertible at all. Intuitively this is becuase the equations</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{Xb} = \boldsymbol{y}
\]</div>
<p>have <em>many</em> possible solutions, being a system of <span class="math notranslate nohighlight">\(n\)</span> equations with <span class="math notranslate nohighlight">\(p+1\)</span> unknowns. One approach in this scenario is to consider the ridge regression solution mentioned above, however here we will describe a different approach.</p>
<p>Any solution <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> to this system will be of the form <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = \boldsymbol{X}^\dagger \boldsymbol{y}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{X}^\dagger\)</span> is a right inverse of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> satisfying</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}\boldsymbol{X}^\dagger = \boldsymbol{I}
\]</div>
<p>since then</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}\hat{\boldsymbol{b}} = \boldsymbol{XX}^\dagger \boldsymbol{y} = \boldsymbol{y}.
\]</div>
<p>Provided the matrix <span class="math notranslate nohighlight">\(\boldsymbol{XX}^\top\)</span> is invertible, one common choice for a right inverse to use is the Moore-Penrose pseudo-inverse, which is given by</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{X}^\dagger = \boldsymbol{X^\top} (\boldsymbol{XX}^\top)^{-1}.
\]</div>
<p>Note this is clearly a right inverse, since <span class="math notranslate nohighlight">\(\boldsymbol{XX}^\dagger = \boldsymbol{XX^\top} (\boldsymbol{XX}^\top)^{-1} = \boldsymbol{I}\)</span>.</p>
<p><strong>Remark.</strong> The pseudo-inverse can actually be defined even when <span class="math notranslate nohighlight">\(\boldsymbol{XX}^\top\)</span> is not invertible: if <span class="math notranslate nohighlight">\(\boldsymbol{X} = \boldsymbol{U\Sigma V}^\top\)</span> is the SVD of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>, then the pseudo-inverse is <span class="math notranslate nohighlight">\(\boldsymbol{X}^\dagger = \boldsymbol{V}\boldsymbol{\Sigma}^\dagger \boldsymbol{U}^\top\)</span> where we define <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}^\dagger\)</span> by taking the reciprocal of all the non-zero elements of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>.</p>
<p>Using the pseudo-inverse we can define the so-called least-norm solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = \boldsymbol{X}^\dagger \boldsymbol{y}\)</span>. It is called the least-norm solution because it satisfies the following property:</p>
<div class="math notranslate nohighlight">
\[
\hat{\boldsymbol{b}} = \text{argmin}_{\boldsymbol{b}: \boldsymbol{Xb} =\boldsymbol{y}} \|\boldsymbol{b}\|.
\]</div>
<p>That is, among all vectors <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span> satisfying <span class="math notranslate nohighlight">\(\boldsymbol{Xb} = \boldsymbol{y}\)</span>, the solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = \boldsymbol{X}^\dagger \boldsymbol{y}\)</span> has the smallest norm.</p>
<p>The pseudo inverse of a matrix <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> can be computed in Python using <code class="docutils literal notranslate"><span class="pre">np.linalg.pinv()</span></code>. Let’s see an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">p</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1">#make the first column all ones</span>

<span class="n">b_star</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">.1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span> <span class="c1">#noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">b_star</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s find <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = \boldsymbol{X}^\dagger \boldsymbol{y}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">b_hat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">X</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we can check how close <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}}\)</span> is to <span class="math notranslate nohighlight">\(\boldsymbol{b}_\star\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">b_hat</span> <span class="o">-</span> <span class="n">b_star</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The error is </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">error</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The error is 7.475752172015447
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="nonlinear-curve-fitting-with-least-squares">
<h2><span class="section-number">8.3.3. </span>Nonlinear curve fitting with least squares<a class="headerlink" href="#nonlinear-curve-fitting-with-least-squares" title="Permalink to this headline">¶</a></h2>
<p>Based on our above discussions, one might be tempted to think that the least squares method can only be used to fit lines to data. However, this is certainly not the case. For example, let’s consider fitting models of the form</p>
<div class="math notranslate nohighlight">
\[
y = f(\boldsymbol{x}) = b_0 + b_1 \phi_1(\boldsymbol{x}) + b_2 \phi_2(\boldsymbol{x}) + \cdots + b_p\phi_p(\boldsymbol{x}) + \varepsilon.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\phi_1,\dots,\phi_p\)</span> can be <em>any</em> non-linear functions. Given <span class="math notranslate nohighlight">\(n\)</span> samples <span class="math notranslate nohighlight">\((\boldsymbol{x}_1,y_1),\dots,(\boldsymbol{x}_n,y_n)\)</span>, let’s define the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Phi} = \begin{bmatrix} 1 &amp; \phi_1(\boldsymbol{x}_1) &amp; \cdots &amp; \phi_p(\boldsymbol{x}_1)\\ 1 &amp; \phi_1(\boldsymbol{x}_2) &amp; \cdots &amp; \phi_p(\boldsymbol{x}_2)\\ \vdots &amp; \cdots &amp; \cdots &amp; \vdots\\ 1 &amp; \phi_1(\boldsymbol{x}_n) &amp; \cdots &amp; \phi_p(\boldsymbol{x}_n)\end{bmatrix}
\end{split}\]</div>
<p>and again <span class="math notranslate nohighlight">\(\boldsymbol{b} = \begin{bmatrix}b_0 &amp; b_1 &amp; \dots &amp; b_p\end{bmatrix}^\top\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y} = \begin{bmatrix}y_1 &amp; \dots &amp; y_n\end{bmatrix}^\top\)</span>. Then we can formulate our problem as</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{y} = \boldsymbol{\Phi b} + \boldsymbol{\varepsilon}
\]</div>
<p>and again we can use the least squares method to find <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}}\)</span> such that <span class="math notranslate nohighlight">\(\boldsymbol{\Phi} \hat{\boldsymbol{b}} \approx  \boldsymbol{y}\)</span>. For example, if <span class="math notranslate nohighlight">\(\boldsymbol{\Phi^\top \Phi}\)</span> is invertible, we could obtain the solution <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = (\boldsymbol{\Phi^\top \Phi})^{-1}\boldsymbol{\Phi^\top y}\)</span>.</p>
<p>A common example of the nonlinear functions <span class="math notranslate nohighlight">\(\phi_j\)</span> that we could use are the polynomials <span class="math notranslate nohighlight">\(\phi_j(x) = x^j\)</span>.</p>
<p><strong>Remark:</strong> As we have seen above, the term “linear regression” is somewhat misleading in describing the types of models that we can fit using least squares. The models that we fit do not need to be linear in the data <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, but rather linear in the <em>parameters</em> <span class="math notranslate nohighlight">\(\boldsymbol{b}\)</span>.</p>
<p>Let’s see a simple example. First, let’s generate some data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">0.05</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_31_0.png" src="../_images/applications_least-squares_31_0.png" />
</div>
</div>
<p>Now let’s write a function to compute the polynomials <span class="math notranslate nohighlight">\(\phi_j(x) = x^j\)</span>. We also define a simple helper function to compute the least squares <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}}\)</span> (note the numpy <code class="docutils literal notranslate"><span class="pre">pinv</span></code> function conveniently computes either <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = \boldsymbol{X]^\top (\boldsymbol{XX}^\top)^{-1}\boldsymbol{y}\)</span> or <span class="math notranslate nohighlight">\(\hat{\boldsymbol{b}} = (\boldsymbol{X^\top X})^{-1}\boldsymbol{X^\top y}\)</span> depending on whether <span class="math notranslate nohighlight">\(\boldsymbol{X^\top X}\)</span> or <span class="math notranslate nohighlight">\(\boldsymbol{XX}^\top\)</span> is invertible).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">p</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">Phi</span><span class="p">[:,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="n">j</span>
    <span class="k">return</span> <span class="n">Phi</span>

<span class="k">def</span> <span class="nf">fit_least_squares</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">Phi</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>First, let’s try fitting a simple first order polynomial to this data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Phi1</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b_hat1</span> <span class="o">=</span> <span class="n">fit_least_squares</span><span class="p">(</span><span class="n">Phi1</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s visualize how well this model actually fits our data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">3.01</span><span class="p">,</span><span class="mf">.01</span><span class="p">)</span>
<span class="n">Phi_test1</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_test1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_test1</span><span class="p">,</span> <span class="n">b_hat1</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;fitted curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_37_0.png" src="../_images/applications_least-squares_37_0.png" />
</div>
</div>
<p>As expected, this doesn’t do very well, since we tried to fit a line to clearly non-linear data. Now let’s try a higher order polynomial, say with <span class="math notranslate nohighlight">\(p=8\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Phi8</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="n">b_hat8</span> <span class="o">=</span> <span class="n">fit_least_squares</span><span class="p">(</span><span class="n">Phi8</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">Phi_test8</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="mi">8</span><span class="p">)</span>
<span class="n">y_test8</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_test8</span><span class="p">,</span> <span class="n">b_hat8</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;fitted curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_39_0.png" src="../_images/applications_least-squares_39_0.png" />
</div>
</div>
<p>This seems to do a better job of capturing the non-linear behavior of the data, but perhaps we can do even better by fitting even more polynomial terms.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Phi20</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">b_hat20</span> <span class="o">=</span> <span class="n">fit_least_squares</span><span class="p">(</span><span class="n">Phi20</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">Phi_test20</span> <span class="o">=</span> <span class="n">polynomial_features</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>
<span class="n">y_test20</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Phi_test20</span><span class="p">,</span> <span class="n">b_hat20</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;fitted curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/applications_least-squares_41_0.png" src="../_images/applications_least-squares_41_0.png" />
</div>
</div>
<p>This does a better job it seems. In practice, one can try many values of <span class="math notranslate nohighlight">\(p\)</span> and use a hold-out test set to determine the best value of <span class="math notranslate nohighlight">\(p\)</span>. This is a topic that you may see later in a statistical machine learning course.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="applications_spectral-clustering.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">8.2. </span>Sprectral Clustering</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="applications_double-descent.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">8.4. </span>The “double descent” phenomenon</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Michael W. Mahoney, N. Benjamin Erichson and Ryan Theisen<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>